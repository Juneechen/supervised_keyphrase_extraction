{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/juneechen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juneechen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/juneechen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# import nltk\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "\n",
    "import util as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word embeddings from the gensim package\n",
    "import gensim.downloader\n",
    "\n",
    "# download the glove embeddings\n",
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../dataset/SciHTC/train_title_abstract_keywords.csv'\n",
    "TEST_PATH = '../dataset/SciHTC/test_title_abstract_keywords.csv'\n",
    "DEV_PATH = '../dataset/SciHTC/dev_title_abstract_keywords.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_matrix shape: (13965, 50)\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 350\n",
    "EMBEDDING_DIM = 50\n",
    "SAMPLE_SIZE = 1000\n",
    "\n",
    "# read train and test data\n",
    "train_df = util.read_data(TRAIN_PATH)\n",
    "test_df = util.read_data(TEST_PATH)\n",
    "\n",
    "input_cols = ['Title', 'Abstract']\n",
    "\n",
    "# process the data and sample 10 for testing\n",
    "train_df = util.preprocess_data(train_df, input_cols, 'Keywords', sample_size=SAMPLE_SIZE)\n",
    "\n",
    "# set up the tokenizer\n",
    "tokenizer = util.setup_tokenizer(train_df, ['input_tokens', 'clean_kp'])\n",
    "\n",
    "# get the embeddings matrix\n",
    "embeddings_matrix = util.get_embeddings_matrix(tokenizer, glove_vectors, EMBEDDING_DIM)\n",
    "print(\"embeddings_matrix shape:\", embeddings_matrix.shape)\n",
    "\n",
    "# create the input array\n",
    "train_X, train_Y = util.create_input_array(train_df, 'input_tokens', 'clean_kp', tokenizer,\n",
    "                                           embeddings_matrix, EMBEDDING_DIM, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length: 305\n",
      "14\n",
      "(1000, 350, 50)\n",
      "[[ 0.29751     0.42748001 -1.10710001 ...  1.1904      0.19129001\n",
      "   0.22145   ]\n",
      " [-0.085318   -0.55786002  0.85042    ...  0.098791    0.17428\n",
      "   0.22194999]\n",
      " [ 0.77967    -0.17454     1.65769994 ... -0.003134   -0.28180999\n",
      "  -0.48699999]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "(1000, 350)\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# find the max length of the input sequences\n",
    "max_length = max([len(seq) for seq in train_df['input_tokens']])\n",
    "print(\"max_length:\", max_length)\n",
    "\n",
    "print(tokenizer.word_index['present'])\n",
    "print(train_X.shape)\n",
    "print(train_X[0])\n",
    "print(train_Y.shape)\n",
    "print(train_Y[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_10 (Bidirect  (None, 350, 128)          58880     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_11 (Bidirect  (None, 128)               98816     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 350)               45150     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 202846 (792.37 KB)\n",
      "Trainable params: 202846 (792.37 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build bi-LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(MAX_LEN, EMBEDDING_DIM)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(MAX_LEN, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 19:13:48.045198: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 9s 161ms/step - loss: 0.3521 - accuracy: 0.1940\n",
      "Epoch 2/3\n",
      "32/32 [==============================] - 4s 136ms/step - loss: 0.0226 - accuracy: 0.5660\n",
      "Epoch 3/3\n",
      "32/32 [==============================] - 4s 136ms/step - loss: 0.0142 - accuracy: 0.7140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2c88a9ad0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(train_X, train_Y, batch_size=32, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 96ms/step\n",
      "pred: ['massive']\n",
      "actual: asic, cad, eda, layout, logic, mooc, vlsi\n",
      "\n",
      "\n",
      "pred: ['and']\n",
      "actual: attitudes, e-participation, gamification, public participation, usage behavior\n",
      "\n",
      "\n",
      "pred: []\n",
      "actual: anonymous, conversation, cues, voting\n",
      "\n",
      "\n",
      "pred: []\n",
      "actual: electromagnetism, evolutionary algorithms, multi-objective optimization, resource-constrained project scheduling\n",
      "\n",
      "\n",
      "pred: ['the']\n",
      "actual: consciousness, constraint, creativity, digital fine art, freedom\n",
      "\n",
      "\n",
      "pred: []\n",
      "actual: energy use, feedback, interaction design, persuasive computing, sustainability, visualization\n",
      "\n",
      "\n",
      "pred: []\n",
      "actual: guided search, model checking, verification\n",
      "\n",
      "\n",
      "pred: []\n",
      "actual: xml, digital preservation, integration, web service\n",
      "\n",
      "\n",
      "pred: ['in', 'an']\n",
      "actual: architecture, software ecosystem, software product lines, variability modeling\n",
      "\n",
      "\n",
      "pred: ['although', 'sensor', 'networks']\n",
      "actual: 3d-localization, delaunay triangulation, map construction, rssi, terrain modeling, wireless sensor networks\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # testing prediction\n",
    "# preds = model.predict(test_x[30:40])\n",
    "\n",
    "# # print(preds[0])\n",
    "\n",
    "# # print prediction\n",
    "# for i in range(len(preds)):\n",
    "#     print(\"pred:\", util.pred_to_keywords(preds[i], test_x[i], tokenizer))\n",
    "#     print(\"actual:\", test_kws[i])\n",
    "#     print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
