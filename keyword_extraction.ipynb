{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import nltk\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "\n",
    "import util as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../dataset/SciHTC/train_title_abstract_keywords.csv'\n",
    "TEST_PATH = '../dataset/SciHTC/test_title_abstract_keywords.csv'\n",
    "DEV_PATH = '../dataset/SciHTC/dev_title_abstract_keywords.csv'\n",
    "\n",
    "MAX_LEN = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = util.read_data(TRAIN_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'present', 'paper', 'discusses', 'how', 'clone', 'sets', 'can', 'be', 'generated', 'from', 'an', 'very', 'large', 'amount', 'of', 'source', 'code', '.', 'The', 'knowledge', 'of', 'clone', 'sets', 'can', 'help', 'to', 'manage', 'software', 'asset', '.', 'For', 'example', ',', 'we', 'can', 'figure', 'out', 'the', 'state', 'of', 'the', 'asset', 'easier', ',', 'or', 'we', 'can', 'build', 'more', 'useful', 'libraries', 'based', 'on', 'the', 'knowledge', '.'], ['Based', 'on', 'the', 'important', 'progresses', 'made', 'in', 'information', 'retrieval', '(', 'IR', ')', 'in', 'terms', 'of', 'theoretical', 'models', 'and', 'evaluations', ',', 'more', 'and', 'more', 'attention', 'has', 'recently', 'been', 'paid', 'to', 'the', 'research', 'in', 'domain', 'specific', 'IR', ',', 'as', 'evidenced', 'by', 'the', 'organization', 'of', 'Genomics', 'and', 'Legal', 'tracks', 'in', 'TREC', '(', 'Text', 'REtrieval', 'Conference', ')', '.', 'We', 'think', 'that', 'now', 'is', 'the', 'right', 'time', 'to', 'carry', 'out', 'large', 'scale', 'evaluations', 'on', 'chemistry', 'datasets', 'in', 'order', 'to', 'promote', 'the', 'research', 'in', 'chemical', 'IR', 'in', 'general', 'and', 'chemical', 'patent', 'IR', 'in', 'particular', '.', 'Accordingly', ',', 'we', 'propose', 'the', 'organization', 'of', 'a', 'chemical', 'IR', 'track', 'in', 'TREC', 'in', 'order', 'to', 'address', 'the', 'challenges', 'in', 'chemical', 'and', 'patent', 'IR', '.', 'In', 'this', 'position', 'paper', ',', 'we', 'present', 'the', 'research', 'questions', 'we', 'will', 'address', 'in', 'the', 'proposed', 'track', ',', 'our', 'initial', 'plan', 'of', 'the', 'proposed', 'track', ',', 'and', 'the', 'kind', 'of', 'search', 'tasks', 'we', 'propose', 'for', 'the', 'track', '.', 'We', 'focus', 'on', 'the', 'design', 'of', 'a', 'new', 'chemical', 'entity', 'search', 'task', 'consisting', 'of', 'two', 'sub-tasks', ',', 'i.e.', ',', 'chemical', 'entity', 'search', 'and', 'chemical', 'entity', 'relation', 'search', '.'], ['Over', 'the', 'past', 'two', 'decades', ',', 'a', 'growing', 'body', 'of', 'research', 'in', 'Borderline', 'Personality', 'Disorder', '(', 'BPD', ')', 'has', 'been', 'accumulated', 'using', 'computer-assisted', 'infield', 'methods', 'to', 'assess', 'self-report', 'data', ',', 'physiological', 'data', 'as', 'well', 'as', 'environmental', 'aspects', 'of', 'BPD', 'patients', 'in', 'everyday', 'life', 'situations', '.', 'In', 'psychological', 'and', 'psychiatric', 'research', ',', 'the', 'assessment', 'method', 'uniting', 'all', 'these', 'aspects', 'of', 'assessment', 'has', 'been', 'coined', '``', 'Ambulatory', 'Assessment', \"''\", '.', 'This', 'method', 'combines', 'many', 'advantages', 'besides', 'real-life', 'and', 'real-time', 'assessment', ',', 'e.g', '.', 'objective', 'and', 'unobtrusive', 'assessments', 'as', 'well', 'as', 'assessments', 'of', 'setting-', 'or', 'context-specificities', '.', 'A', 'seminal', 'field', 'of', 'application', 'is', 'the', 'use', 'of', 'electronic', 'devices', 'as', 'therapeutic', 'tools', ',', 'giving', 'interactive', 'feedback', 'to', 'patients', ',', 'i.e', '.', 'providing', 'interventions', 'at', 'the', 'very', 'moments', 'when', 'patients', 'are', 'most', 'in', 'need', 'of', 'additional', 'support', '.', 'By', 'giving', 'two', 'examples', 'of', 'BPD', 'diagnostic', 'criteria', ',', 'we', 'outline', 'the', 'key', 'technical', 'needs', 'in', 'order', 'to', 'take', 'the', 'use', 'of', 'sensor', 'technology', 'in', 'psychological', 'research', 'and', 'therapy', 'to', 'a', 'higher', 'level', '.', 'The', 'following', 'key', 'technical', 'needs', 'have', 'been', 'identified', ':', '(', '1', ')', 'tools', 'for', 'emotion', 'detection', 'providing', 'an', 'objective', 'measure', 'of', 'current', 'emotional', 'states', ';', '(', '2', ')', 'tools', 'for', 'identifying', 'persons', ';', '(', '3', ')', 'tools', 'for', 'detecting', 'instances', 'of', 'self-destructive', 'behavior', 'like', 'alcohol', 'or', 'drug', 'use', 'or', 'speeding/reckless-driving', ';', '(', '4', ')', 'tools', 'for', 'integrating', 'and', 'analyzing', 'information', 'of', 'various', 'sensors', 'in', 'order', 'to', 'give', 'automated', 'feedback', 'to', 'the', 'patient', 'in', 'real', 'time', '.', 'In', 'a', 'nutshell', ',', 'even', 'though', 'Ambulatory', 'Assessment', 'has', 'been', 'successfully', 'used', 'in', 'BPD', 'research', 'over', 'the', 'past', 'few', 'years', ',', 'the', 'method', 'may', 'offer', 'many', 'more', 'options', 'of', 'applications', 'in', 'research', 'and', 'even', 'more', 'in', 'therapy', '.', 'However', ',', 'psychologists', 'and', 'psychiatrists', 'lack', 'the', 'technological', 'knowledge', 'to', 'realize', 'possible', 'new', 'applications', 'of', 'Ambulatory', 'Assessment', '.', 'In', 'order', 'to', 'increase', 'the', 'usefulness', 'of', 'Ambulatory', 'Assessment', 'in', 'psychological', 'research', 'and', 'therapy', ',', 'multidisciplinary', 'teams', ',', 'including', 'technology', 'experts', 'as', 'well', 'as', 'clinical', 'psychologists', ',', 'are', 'clearly', 'needed', '.'], ['Refactoring', 'is', 'an', 'important', 'way', 'to', 'improve', 'the', 'design', 'of', 'existing', 'code', '.', 'Identifying', 'refactoring', 'opportunities', '(', 'i.e.', ',', 'code', 'fragments', 'that', 'can', 'be', 'refactored', ')', 'in', 'large', 'code', 'bases', 'is', 'a', 'challenging', 'task', '.', 'In', 'this', 'paper', ',', 'we', 'propose', 'a', 'novel', ',', 'automated', 'and', 'scalable', 'technique', 'for', 'identifying', 'cross-function', 'refactoring', 'opportunities', 'that', 'span', 'more', 'than', 'one', 'function', '(', 'e.g.', ',', 'Extract', 'Method', 'and', 'Inline', 'Method', ')', '.', 'The', 'key', 'of', 'our', 'technique', 'is', 'the', 'design', 'of', 'efficient', 'vector', 'inlining', 'operations', 'that', 'emulate', 'the', 'effect', 'of', 'method', 'inlining', 'among', 'code', 'fragments', ',', 'so', 'that', 'the', 'problem', 'of', 'identifying', 'cross-function', 'refactoring', 'can', 'be', 'reduced', 'to', 'the', 'problem', 'of', 'finding', 'similar', 'vectors', 'before', 'and', 'after', 'inlining', '.', 'We', 'have', 'implemented', 'our', 'technique', 'in', 'a', 'prototype', 'tool', 'named', 'ReDex', 'which', 'encodes', 'Java', 'programs', 'to', 'particular', 'vectors', '.', 'We', 'have', 'applied', 'the', 'tool', 'to', 'a', 'large', 'code', 'base', ',', '4.5', 'million', 'lines', 'of', 'code', ',', 'comprising', 'of', '200', 'bundle', 'projects', 'in', 'the', 'Eclipse', 'ecosystem', '(', 'e.g.', ',', 'Eclipse', 'JDT', ',', 'Eclipse', 'PDE', ',', 'Apache', 'Commons', ',', 'Hamcrest', ',', 'etc.', ')', '.', 'Also', ',', 'different', 'from', 'many', 'other', 'studies', 'on', 'detecting', 'refactoring', ',', 'ReDex', 'only', 'searches', 'for', 'code', 'fragments', 'that', 'can', 'be', ',', 'but', 'have', 'not', 'yet', 'been', ',', 'refactored', 'in', 'a', 'way', 'similar', 'to', 'some', 'refactoring', 'that', 'happened', 'in', 'the', 'code', 'base', '.', 'Our', 'results', 'show', 'that', 'ReDex', 'can', 'find', '277', 'cross-function', 'refactoring', 'opportunities', 'in', '2', 'minutes', ',', 'and', '223', 'cases', 'were', 'labelled', 'as', 'true', 'opportunities', 'by', 'users', ',', 'and', 'cover', 'many', 'categories', 'of', 'cross-function', 'refactoring', 'operations', 'in', 'classical', 'refactoring', 'books', ',', 'such', 'as', 'Self', 'Encapsulate', 'Field', ',', 'Decompose', 'Conditional', 'Expression', ',', 'Hide', 'Delegate', ',', 'Preserve', 'Whole', 'Object', ',', 'etc', '.'], ['Productivity', 'is', 'the', 'main', 'concern', 'of', 'today', \"'s\", 'IT', 'enabled', 'office', 'environments', '.', 'Besides', 'other', 'inter-related', 'factors', ',', 'the', 'productivity', 'of', 'an', 'individual', 'or', 'a', 'group', 'is', 'mainly', 'based', 'upon', 'the', 'end', 'user', \"'s\", 'tasks', 'in', 'a', 'specific', 'domain', '.', 'These', 'days', ',', 'the', 'end', 'users', \"'\", 'information', 'management', 'tasks', 'are', 'being', 'performed', 'semi-automatically', '.', 'However', ',', 'the', 'record', 'of', 'the', 'tasks', 'maintained', 'is', 'insufficient', 'for', 'an', 'exhaustive', 'tracking', 'and', 'monitoring', '.', 'This', 'results', 'in', 'a', 'situation', 'where', 'although', 'the', 'same', 'tasks', 'are', 'repeatedly', 'performed', 'in', 'parallel', 'or', 'in', 'sequence', 'by', 'different', 'users', 'but', 'the', 'systems', 'are', 'unable', 'to', 'coordinate', 'for', 'helping', 'the', 'users', 'for', 'smooth', 'and', 'swift', 'task', 'execution', '.', 'Task', 'is', 'part', 'of', 'the', 'context', 'and', 'to', 'track', 'the', 'task', 'its', 'conceptual', 'model', ',', 'that', 'will', 'be', 'interconnected', 'with', 'other', 'contextual', 'components', ',', 'is', 'proposed', '.', 'We', 'propose', 'ontology', 'to', 'realize', 'the', 'task', 'model', 'that', 'will', 'help', 'in', 'making', 'an', 'application', 'to', 'measure', 'and', 'increase', 'one', \"'s\", 'productivity', 'in', 'the', 'academic', 'domain', '.']]\n",
      "0              code clone, reengineering for libraries\n",
      "1    chemical information retrieval, chemistry, pat...\n",
      "2    ambulatory assessment, borderline personality ...\n",
      "3    refactoring, software evolution, vector-based ...\n",
      "4    information context, productivity, task, task ...\n",
      "Name: Keywords, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# apply clean_keywords to each row of df['Keywords']\n",
    "df['Keywords'] = df['Keywords'].apply(util.clean_keywords)\n",
    "\n",
    "# test on the first 5 row of abstract and keywords\n",
    "abstract = util.tokenize_sentence(df.head(5)['Abstract'])\n",
    "keywords = df.head(5)['Keywords']\n",
    "\n",
    "print(abstract)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyword_phrase: code clone, reengineering for libraries\n",
      "phrases: ['code clone', 'reengineering for libraries']\n",
      "phrase_tokens: [[16, 77], [10, 151]]\n",
      "token: 16\n",
      "at: 17\n",
      "token: 77\n",
      "at: 5\n",
      "token: 10\n",
      "at: 31\n",
      "token: 151\n",
      "at: 51\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "keyword_phrase: chemical information retrieval, chemistry, patent retrieval, prior art search\n",
      "phrases: ['chemical information retrieval', 'chemistry', 'patent retrieval', 'prior art search']\n",
      "phrase_tokens: [[20, 54, 85], [171], [91, 85], [42]]\n",
      "token: 20\n",
      "at: 78\n",
      "token: 54\n",
      "at: 7\n",
      "token: 85\n",
      "at: 8\n",
      "token: 171\n",
      "at: 69\n",
      "token: 91\n",
      "at: 84\n",
      "token: 85\n",
      "at: 8\n",
      "token: 42\n",
      "at: 144\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "keyword_phrase: ambulatory assessment, borderline personality disorder, clinical psychology, everyday life, interactive feedback, real-time data interpretation, unobtrusive objective assessment\n",
      "phrases: ['ambulatory assessment', 'borderline personality disorder', 'clinical psychology', 'everyday life', 'interactive feedback', 'real-time data interpretation', 'unobtrusive objective assessment']\n",
      "phrase_tokens: [[44, 21], [189, 190, 191], [284], [201, 202], [222, 107], [265, 90, 98], [215, 102, 21]]\n",
      "token: 44\n",
      "at: 64\n",
      "token: 21\n",
      "at: 52\n",
      "token: 189\n",
      "at: 12\n",
      "token: 190\n",
      "at: 13\n",
      "token: 191\n",
      "at: 14\n",
      "token: 284\n",
      "at: 323\n",
      "token: 201\n",
      "at: 41\n",
      "token: 202\n",
      "at: 42\n",
      "token: 222\n",
      "at: 110\n",
      "token: 107\n",
      "at: 111\n",
      "token: 265\n",
      "at: 240\n",
      "token: 90\n",
      "at: 241\n",
      "token: 98\n",
      "at: 28\n",
      "token: 215\n",
      "at: 83\n",
      "token: 102\n",
      "at: 81\n",
      "token: 21\n",
      "at: 52\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "keyword_phrase: refactoring, software evolution, vector-based representation\n",
      "phrases: ['refactoring', 'software evolution', 'vector-based representation']\n",
      "phrase_tokens: [[18], [144], [299, 53]]\n",
      "token: 18\n",
      "at: 0\n",
      "token: 299\n",
      "at: 79\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "keyword_phrase: information context, productivity, task, task modeling\n",
      "phrases: ['information context', 'productivity', 'task', 'task modeling']\n",
      "phrase_tokens: [[54, 403], [74], [27], [27]]\n",
      "token: 54\n",
      "at: 47\n",
      "token: 403\n",
      "at: 116\n",
      "token: 74\n",
      "at: 0\n",
      "token: 27\n",
      "at: 108\n",
      "token: 27\n",
      "at: 108\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set up tokenizer \n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(abstract)\n",
    "sequences = tokenizer.texts_to_sequences(abstract)\n",
    "\n",
    "# set up train sequences (with padding)\n",
    "train_x = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_LEN, padding='post')\n",
    "# set up train labels\n",
    "train_y = util.keywords_marking(keywords, sequences, 350, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 76, 51, 138, 139, 77, 78, 19, 29, 140, 79, 22, 80, 36, 141, 4, 142, 16, 5, 1, 52, 4, 77, 78, 19, 81, 7, 143, 144, 82, 5, 10, 145, 2, 9, 19, 146, 83, 1, 147, 4, 1, 82, 148, 2, 23, 9, 19, 149, 24, 150, 151, 53, 30, 1, 52, 5], [53, 30, 1, 84, 152, 153, 3, 54, 85, 11, 25, 12, 3, 154, 4, 155, 156, 6, 86, 2, 24, 6, 24, 157, 37, 158, 26, 159, 7, 1, 17, 3, 55, 87, 25, 2, 13, 160, 38, 1, 88, 4, 161, 6, 162, 163, 3, 89, 11, 164, 85, 165, 12, 5, 9, 166, 14, 167, 15, 1, 168, 90, 7, 169, 83, 36, 170, 86, 30, 171, 172, 3, 31, 7, 173, 1, 17, 3, 20, 25, 3, 174, 6, 20, 91, 25, 3, 92, 5, 175, 2, 9, 39, 1, 88, 4, 8, 20, 25, 32, 3, 89, 3, 31, 7, 93, 1, 176, 3, 20, 6, 91, 25, 5, 3, 40, 177, 51, 2, 9, 76, 1, 17, 178, 9, 56, 93, 3, 1, 57, 32, 2, 41, 179, 180, 4, 1, 57, 32, 2, 6, 1, 181, 4, 42, 33, 9, 39, 10, 1, 32, 5, 9, 182, 30, 1, 58, 4, 8, 94, 20, 59, 42, 27, 183, 4, 60, 184, 2, 95, 2, 20, 59, 42, 6, 20, 59, 185, 42, 5], [96, 1, 97, 60, 186, 2, 8, 187, 188, 4, 17, 3, 189, 190, 191, 11, 43, 12, 37, 26, 192, 193, 194, 195, 196, 7, 197, 198, 98, 2, 199, 98, 13, 61, 13, 200, 99, 4, 43, 62, 3, 201, 202, 203, 5, 3, 63, 6, 204, 17, 2, 1, 21, 28, 205, 206, 100, 99, 4, 21, 37, 26, 207, 208, 44, 21, 209, 5, 40, 28, 210, 45, 211, 101, 212, 6, 213, 21, 2, 214, 5, 102, 6, 215, 103, 13, 61, 13, 103, 4, 216, 23, 217, 5, 8, 218, 104, 4, 105, 15, 1, 64, 4, 219, 220, 13, 221, 34, 2, 106, 222, 107, 7, 62, 2, 223, 5, 108, 224, 225, 1, 80, 226, 227, 62, 35, 228, 3, 229, 4, 230, 231, 5, 38, 106, 60, 232, 4, 43, 233, 234, 2, 9, 235, 1, 65, 109, 110, 3, 31, 7, 236, 1, 64, 4, 237, 111, 3, 63, 17, 6, 66, 7, 8, 238, 239, 5, 1, 240, 65, 109, 110, 46, 26, 241, 242, 11, 243, 12, 34, 10, 244, 245, 108, 22, 102, 112, 4, 246, 247, 248, 67, 11, 113, 12, 34, 10, 47, 249, 67, 11, 250, 12, 34, 10, 114, 251, 4, 252, 253, 254, 255, 23, 256, 64, 23, 257, 67, 11, 258, 12, 34, 10, 259, 6, 260, 54, 4, 261, 262, 3, 31, 7, 263, 115, 107, 7, 1, 264, 3, 265, 90, 5, 3, 8, 266, 2, 116, 267, 44, 21, 37, 26, 268, 269, 3, 43, 17, 96, 1, 97, 270, 271, 2, 1, 28, 272, 273, 45, 24, 274, 4, 117, 3, 17, 6, 116, 24, 3, 66, 5, 118, 2, 119, 6, 275, 276, 1, 277, 52, 7, 120, 278, 94, 117, 4, 44, 21, 5, 3, 31, 7, 121, 1, 279, 4, 44, 21, 3, 63, 17, 6, 66, 2, 280, 281, 2, 282, 111, 283, 13, 61, 13, 284, 119, 2, 35, 285, 286, 5], [18, 15, 22, 84, 122, 7, 287, 1, 58, 4, 288, 16, 5, 47, 18, 48, 11, 95, 2, 16, 68, 14, 19, 29, 123, 12, 3, 36, 16, 289, 15, 8, 290, 27, 5, 3, 40, 51, 2, 9, 39, 8, 291, 2, 115, 6, 292, 69, 10, 47, 49, 18, 48, 14, 293, 24, 294, 124, 295, 11, 125, 2, 296, 28, 6, 297, 28, 12, 5, 1, 65, 4, 41, 69, 15, 1, 58, 4, 298, 299, 70, 126, 14, 300, 1, 301, 4, 28, 70, 302, 16, 68, 2, 303, 14, 1, 127, 4, 47, 49, 18, 19, 29, 304, 7, 1, 127, 4, 305, 128, 129, 306, 6, 307, 70, 5, 9, 46, 308, 41, 69, 3, 8, 309, 130, 310, 71, 311, 312, 313, 314, 7, 92, 129, 5, 9, 46, 315, 1, 130, 7, 8, 36, 16, 131, 2, 316, 317, 318, 4, 16, 2, 319, 4, 320, 321, 322, 3, 1, 72, 323, 11, 125, 2, 72, 324, 2, 72, 325, 2, 326, 327, 2, 328, 2, 329, 12, 5, 330, 2, 132, 79, 45, 73, 331, 30, 114, 18, 2, 71, 332, 333, 10, 16, 68, 14, 19, 29, 2, 133, 46, 334, 335, 26, 2, 123, 3, 8, 122, 128, 7, 336, 18, 14, 337, 3, 1, 16, 131, 5, 41, 134, 338, 14, 71, 19, 339, 340, 49, 18, 48, 3, 113, 341, 2, 6, 342, 343, 344, 345, 13, 346, 48, 38, 50, 2, 6, 347, 45, 348, 4, 49, 18, 126, 3, 349, 18, 350, 2, 351, 13, 352, 353, 104, 2, 354, 355, 356, 2, 357, 358, 2, 359, 360, 361, 2, 362, 5], [74, 15, 1, 363, 364, 4, 365, 75, 366, 367, 368, 369, 5, 101, 73, 370, 371, 2, 1, 74, 4, 22, 372, 23, 8, 373, 15, 374, 53, 375, 1, 135, 376, 75, 33, 3, 8, 87, 55, 5, 100, 377, 2, 1, 135, 50, 378, 54, 379, 33, 35, 380, 136, 381, 5, 118, 2, 1, 382, 4, 1, 33, 383, 15, 384, 10, 22, 385, 386, 6, 387, 5, 40, 134, 3, 8, 388, 389, 390, 1, 391, 33, 35, 392, 136, 3, 393, 23, 3, 394, 38, 132, 50, 133, 1, 395, 35, 396, 7, 397, 10, 398, 1, 50, 10, 399, 6, 400, 27, 401, 5, 27, 15, 402, 4, 1, 403, 6, 7, 32, 1, 27, 404, 405, 137, 2, 14, 56, 29, 406, 407, 73, 408, 409, 2, 15, 57, 5, 9, 39, 410, 7, 120, 1, 27, 137, 14, 56, 81, 3, 411, 22, 105, 7, 112, 6, 121, 124, 75, 74, 3, 1, 412, 55, 5]]\n",
      "330\n",
      "76\n",
      "paper\n",
      "[1, 76, 51, 138, 139, 77, 78, 19, 29, 140, 79, 22, 80, 36, 141, 4, 142, 16, 5, 1, 52, 4, 77, 78, 19, 81, 7, 143, 144, 82, 5, 10, 145, 2, 9, 19, 146, 83, 1, 147, 4, 1, 82, 148, 2, 23, 9, 19, 149, 24, 150, 151, 53, 30, 1, 52, 5]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# run some checks\n",
    "print(sequences)\n",
    "\n",
    "# print(tokenizer.word_index)\n",
    "\n",
    "# find the max length of the sequences\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "print(max_length)\n",
    "\n",
    "# find index of a word \n",
    "print(tokenizer.word_index['present'])\n",
    "# find word at index \n",
    "print(tokenizer.index_word[51])\n",
    "\n",
    "print(sequences[0])\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
