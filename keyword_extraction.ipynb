{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# import nltk\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "\n",
    "import util as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../dataset/SciHTC/train_title_abstract_keywords.csv'\n",
    "TEST_PATH = '../dataset/SciHTC/test_title_abstract_keywords.csv'\n",
    "DEV_PATH = '../dataset/SciHTC/dev_title_abstract_keywords.csv'\n",
    "\n",
    "MAX_LEN = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train and test data\n",
    "train_df = util.read_data(TRAIN_PATH)\n",
    "test_df = util.read_data(TEST_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply clean_keywords to each row of df['Keywords']\n",
    "train_df['Keywords'] = train_df['Keywords'].apply(util.clean_keywords)\n",
    "test_df['Keywords'] = test_df['Keywords'].apply(util.clean_keywords)\n",
    "\n",
    "# set up train data and labels, test on part of the train samples\n",
    "train_abs = util.tokenize_sentence(train_df.head(50000)['Abstract'])\n",
    "train_kws = train_df.head(50000)['Keywords']\n",
    "\n",
    "# set up train data and labels\n",
    "# train_abs = util.tokenize_sentence(train_df['Abstract'])\n",
    "# train_kws = train_df['Keywords']\n",
    "\n",
    "# set up test data and labels\n",
    "test_abs = util.tokenize_sentence(test_df['Abstract'])\n",
    "test_kws = test_df['Keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'present', 'paper', 'discusses', 'how', 'clone', 'sets', 'can', 'be', 'generated', 'from', 'an', 'very', 'large', 'amount', 'of', 'source', 'code', '.', 'The', 'knowledge', 'of', 'clone', 'sets', 'can', 'help', 'to', 'manage', 'software', 'asset', '.', 'For', 'example', ',', 'we', 'can', 'figure', 'out', 'the', 'state', 'of', 'the', 'asset', 'easier', ',', 'or', 'we', 'can', 'build', 'more', 'useful', 'libraries', 'based', 'on', 'the', 'knowledge', '.'], ['Based', 'on', 'the', 'important', 'progresses', 'made', 'in', 'information', 'retrieval', '(', 'IR', ')', 'in', 'terms', 'of', 'theoretical', 'models', 'and', 'evaluations', ',', 'more', 'and', 'more', 'attention', 'has', 'recently', 'been', 'paid', 'to', 'the', 'research', 'in', 'domain', 'specific', 'IR', ',', 'as', 'evidenced', 'by', 'the', 'organization', 'of', 'Genomics', 'and', 'Legal', 'tracks', 'in', 'TREC', '(', 'Text', 'REtrieval', 'Conference', ')', '.', 'We', 'think', 'that', 'now', 'is', 'the', 'right', 'time', 'to', 'carry', 'out', 'large', 'scale', 'evaluations', 'on', 'chemistry', 'datasets', 'in', 'order', 'to', 'promote', 'the', 'research', 'in', 'chemical', 'IR', 'in', 'general', 'and', 'chemical', 'patent', 'IR', 'in', 'particular', '.', 'Accordingly', ',', 'we', 'propose', 'the', 'organization', 'of', 'a', 'chemical', 'IR', 'track', 'in', 'TREC', 'in', 'order', 'to', 'address', 'the', 'challenges', 'in', 'chemical', 'and', 'patent', 'IR', '.', 'In', 'this', 'position', 'paper', ',', 'we', 'present', 'the', 'research', 'questions', 'we', 'will', 'address', 'in', 'the', 'proposed', 'track', ',', 'our', 'initial', 'plan', 'of', 'the', 'proposed', 'track', ',', 'and', 'the', 'kind', 'of', 'search', 'tasks', 'we', 'propose', 'for', 'the', 'track', '.', 'We', 'focus', 'on', 'the', 'design', 'of', 'a', 'new', 'chemical', 'entity', 'search', 'task', 'consisting', 'of', 'two', 'sub-tasks', ',', 'i.e.', ',', 'chemical', 'entity', 'search', 'and', 'chemical', 'entity', 'relation', 'search', '.'], ['Over', 'the', 'past', 'two', 'decades', ',', 'a', 'growing', 'body', 'of', 'research', 'in', 'Borderline', 'Personality', 'Disorder', '(', 'BPD', ')', 'has', 'been', 'accumulated', 'using', 'computer-assisted', 'infield', 'methods', 'to', 'assess', 'self-report', 'data', ',', 'physiological', 'data', 'as', 'well', 'as', 'environmental', 'aspects', 'of', 'BPD', 'patients', 'in', 'everyday', 'life', 'situations', '.', 'In', 'psychological', 'and', 'psychiatric', 'research', ',', 'the', 'assessment', 'method', 'uniting', 'all', 'these', 'aspects', 'of', 'assessment', 'has', 'been', 'coined', '``', 'Ambulatory', 'Assessment', \"''\", '.', 'This', 'method', 'combines', 'many', 'advantages', 'besides', 'real-life', 'and', 'real-time', 'assessment', ',', 'e.g', '.', 'objective', 'and', 'unobtrusive', 'assessments', 'as', 'well', 'as', 'assessments', 'of', 'setting-', 'or', 'context-specificities', '.', 'A', 'seminal', 'field', 'of', 'application', 'is', 'the', 'use', 'of', 'electronic', 'devices', 'as', 'therapeutic', 'tools', ',', 'giving', 'interactive', 'feedback', 'to', 'patients', ',', 'i.e', '.', 'providing', 'interventions', 'at', 'the', 'very', 'moments', 'when', 'patients', 'are', 'most', 'in', 'need', 'of', 'additional', 'support', '.', 'By', 'giving', 'two', 'examples', 'of', 'BPD', 'diagnostic', 'criteria', ',', 'we', 'outline', 'the', 'key', 'technical', 'needs', 'in', 'order', 'to', 'take', 'the', 'use', 'of', 'sensor', 'technology', 'in', 'psychological', 'research', 'and', 'therapy', 'to', 'a', 'higher', 'level', '.', 'The', 'following', 'key', 'technical', 'needs', 'have', 'been', 'identified', ':', '(', '1', ')', 'tools', 'for', 'emotion', 'detection', 'providing', 'an', 'objective', 'measure', 'of', 'current', 'emotional', 'states', ';', '(', '2', ')', 'tools', 'for', 'identifying', 'persons', ';', '(', '3', ')', 'tools', 'for', 'detecting', 'instances', 'of', 'self-destructive', 'behavior', 'like', 'alcohol', 'or', 'drug', 'use', 'or', 'speeding/reckless-driving', ';', '(', '4', ')', 'tools', 'for', 'integrating', 'and', 'analyzing', 'information', 'of', 'various', 'sensors', 'in', 'order', 'to', 'give', 'automated', 'feedback', 'to', 'the', 'patient', 'in', 'real', 'time', '.', 'In', 'a', 'nutshell', ',', 'even', 'though', 'Ambulatory', 'Assessment', 'has', 'been', 'successfully', 'used', 'in', 'BPD', 'research', 'over', 'the', 'past', 'few', 'years', ',', 'the', 'method', 'may', 'offer', 'many', 'more', 'options', 'of', 'applications', 'in', 'research', 'and', 'even', 'more', 'in', 'therapy', '.', 'However', ',', 'psychologists', 'and', 'psychiatrists', 'lack', 'the', 'technological', 'knowledge', 'to', 'realize', 'possible', 'new', 'applications', 'of', 'Ambulatory', 'Assessment', '.', 'In', 'order', 'to', 'increase', 'the', 'usefulness', 'of', 'Ambulatory', 'Assessment', 'in', 'psychological', 'research', 'and', 'therapy', ',', 'multidisciplinary', 'teams', ',', 'including', 'technology', 'experts', 'as', 'well', 'as', 'clinical', 'psychologists', ',', 'are', 'clearly', 'needed', '.'], ['Refactoring', 'is', 'an', 'important', 'way', 'to', 'improve', 'the', 'design', 'of', 'existing', 'code', '.', 'Identifying', 'refactoring', 'opportunities', '(', 'i.e.', ',', 'code', 'fragments', 'that', 'can', 'be', 'refactored', ')', 'in', 'large', 'code', 'bases', 'is', 'a', 'challenging', 'task', '.', 'In', 'this', 'paper', ',', 'we', 'propose', 'a', 'novel', ',', 'automated', 'and', 'scalable', 'technique', 'for', 'identifying', 'cross-function', 'refactoring', 'opportunities', 'that', 'span', 'more', 'than', 'one', 'function', '(', 'e.g.', ',', 'Extract', 'Method', 'and', 'Inline', 'Method', ')', '.', 'The', 'key', 'of', 'our', 'technique', 'is', 'the', 'design', 'of', 'efficient', 'vector', 'inlining', 'operations', 'that', 'emulate', 'the', 'effect', 'of', 'method', 'inlining', 'among', 'code', 'fragments', ',', 'so', 'that', 'the', 'problem', 'of', 'identifying', 'cross-function', 'refactoring', 'can', 'be', 'reduced', 'to', 'the', 'problem', 'of', 'finding', 'similar', 'vectors', 'before', 'and', 'after', 'inlining', '.', 'We', 'have', 'implemented', 'our', 'technique', 'in', 'a', 'prototype', 'tool', 'named', 'ReDex', 'which', 'encodes', 'Java', 'programs', 'to', 'particular', 'vectors', '.', 'We', 'have', 'applied', 'the', 'tool', 'to', 'a', 'large', 'code', 'base', ',', '4.5', 'million', 'lines', 'of', 'code', ',', 'comprising', 'of', '200', 'bundle', 'projects', 'in', 'the', 'Eclipse', 'ecosystem', '(', 'e.g.', ',', 'Eclipse', 'JDT', ',', 'Eclipse', 'PDE', ',', 'Apache', 'Commons', ',', 'Hamcrest', ',', 'etc.', ')', '.', 'Also', ',', 'different', 'from', 'many', 'other', 'studies', 'on', 'detecting', 'refactoring', ',', 'ReDex', 'only', 'searches', 'for', 'code', 'fragments', 'that', 'can', 'be', ',', 'but', 'have', 'not', 'yet', 'been', ',', 'refactored', 'in', 'a', 'way', 'similar', 'to', 'some', 'refactoring', 'that', 'happened', 'in', 'the', 'code', 'base', '.', 'Our', 'results', 'show', 'that', 'ReDex', 'can', 'find', '277', 'cross-function', 'refactoring', 'opportunities', 'in', '2', 'minutes', ',', 'and', '223', 'cases', 'were', 'labelled', 'as', 'true', 'opportunities', 'by', 'users', ',', 'and', 'cover', 'many', 'categories', 'of', 'cross-function', 'refactoring', 'operations', 'in', 'classical', 'refactoring', 'books', ',', 'such', 'as', 'Self', 'Encapsulate', 'Field', ',', 'Decompose', 'Conditional', 'Expression', ',', 'Hide', 'Delegate', ',', 'Preserve', 'Whole', 'Object', ',', 'etc', '.'], ['Productivity', 'is', 'the', 'main', 'concern', 'of', 'today', \"'s\", 'IT', 'enabled', 'office', 'environments', '.', 'Besides', 'other', 'inter-related', 'factors', ',', 'the', 'productivity', 'of', 'an', 'individual', 'or', 'a', 'group', 'is', 'mainly', 'based', 'upon', 'the', 'end', 'user', \"'s\", 'tasks', 'in', 'a', 'specific', 'domain', '.', 'These', 'days', ',', 'the', 'end', 'users', \"'\", 'information', 'management', 'tasks', 'are', 'being', 'performed', 'semi-automatically', '.', 'However', ',', 'the', 'record', 'of', 'the', 'tasks', 'maintained', 'is', 'insufficient', 'for', 'an', 'exhaustive', 'tracking', 'and', 'monitoring', '.', 'This', 'results', 'in', 'a', 'situation', 'where', 'although', 'the', 'same', 'tasks', 'are', 'repeatedly', 'performed', 'in', 'parallel', 'or', 'in', 'sequence', 'by', 'different', 'users', 'but', 'the', 'systems', 'are', 'unable', 'to', 'coordinate', 'for', 'helping', 'the', 'users', 'for', 'smooth', 'and', 'swift', 'task', 'execution', '.', 'Task', 'is', 'part', 'of', 'the', 'context', 'and', 'to', 'track', 'the', 'task', 'its', 'conceptual', 'model', ',', 'that', 'will', 'be', 'interconnected', 'with', 'other', 'contextual', 'components', ',', 'is', 'proposed', '.', 'We', 'propose', 'ontology', 'to', 'realize', 'the', 'task', 'model', 'that', 'will', 'help', 'in', 'making', 'an', 'application', 'to', 'measure', 'and', 'increase', 'one', \"'s\", 'productivity', 'in', 'the', 'academic', 'domain', '.']]\n",
      "0              code clone, reengineering for libraries\n",
      "1    chemical information retrieval, chemistry, pat...\n",
      "2    ambulatory assessment, borderline personality ...\n",
      "3    refactoring, software evolution, vector-based ...\n",
      "4    information context, productivity, task, task ...\n",
      "Name: Keywords, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_abs[:5])\n",
    "print(train_kws[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "50000\n",
      "18616\n",
      "18616\n"
     ]
    }
   ],
   "source": [
    "print(len(train_abs))\n",
    "print(len(train_kws))\n",
    "print(len(test_abs))\n",
    "print(len(test_kws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up tokenizer, fit on both train and test data\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train_abs)\n",
    "tokenizer.fit_on_texts(test_abs)\n",
    "\n",
    "# set up train sequences (with padding)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_abs)\n",
    "train_x = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=MAX_LEN, padding='post')\n",
    "# set up train labels\n",
    "train_y = util.keywords_marking(train_kws, train_x, 350, tokenizer)\n",
    "\n",
    "# set up test sequences (with padding)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_abs)\n",
    "test_x = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=MAX_LEN, padding='post')\n",
    "# set up test labels\n",
    "test_y = util.keywords_marking(test_kws, test_x, 350, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 57, 25, 1419, 59, 5163, 494, 22, 23, 484, 27, 20, 209, 110, 502, 4, 311, 140, 2, 1, 153, 4, 5163, 494, 22, 284, 6, 1290, 73, 5302, 2, 10, 357, 3, 9, 22, 4217, 343, 1, 314, 4, 1, 5302, 1908, 3, 34, 9, 22, 545, 54, 431, 1379, 39, 14, 1, 153, 2], [39, 14, 1, 134, 8113, 518, 8, 37, 321, 17, 2082, 15, 8, 260, 4, 829, 106, 5, 1305, 3, 54, 5, 54, 516, 41, 537, 68, 3584, 6, 1, 81, 8, 308, 268, 2082, 3, 18, 9112, 21, 1, 1176, 4, 7241, 5, 2562, 3022, 8, 3417, 17, 327, 321, 2258, 15, 2, 9, 3157, 12, 976, 11, 1, 1616, 61, 6, 2355, 343, 110, 475, 1305, 14, 7519, 533, 8, 135, 6, 2111, 1, 81, 8, 4917, 2082, 8, 281, 5, 4917, 4625, 2082, 8, 223, 2, 2969, 3, 9, 77, 1, 1176, 4, 7, 4917, 2082, 1314, 8, 3417, 8, 135, 6, 258, 1, 222, 8, 4917, 5, 4625, 2082, 2, 8, 13, 906, 25, 3, 9, 57, 1, 81, 631, 9, 90, 258, 8, 1, 72, 1314, 3, 24, 571, 1635, 4, 1, 72, 1314, 3, 5, 1, 1360, 4, 108, 218, 9, 77, 10, 1, 1314, 2, 9, 285, 14, 1, 40, 4, 7, 53, 4917, 1465, 108, 162, 1782, 4, 66, 11820, 3, 594, 3, 4917, 1465, 108, 5, 4917, 1465, 1293, 108, 2], [97, 1, 809, 66, 2742, 3, 7, 923, 840, 4, 81, 8, 20169, 2665, 4978, 17, 20170, 15, 41, 68, 6134, 33, 10899, 70727, 111, 6, 1301, 9014, 26, 3, 3139, 26, 18, 120, 18, 1563, 456, 4, 20170, 1387, 8, 1543, 799, 1061, 2, 8, 3244, 5, 19251, 81, 3, 1, 946, 70, 40010, 112, 35, 456, 4, 946, 41, 68, 10174, 122, 12729, 946, 121, 2, 13, 70, 1096, 102, 903, 1977, 2662, 5, 378, 946, 3, 729, 2, 726, 5, 5609, 3604, 18, 120, 18, 3604, 4, 70728, 34, 70729, 2, 7, 7580, 342, 4, 101, 11, 1, 50, 4, 1181, 143, 18, 6418, 172, 3, 2374, 243, 349, 6, 1387, 3, 1087, 2, 410, 2845, 47, 1, 209, 4736, 83, 1387, 19, 114, 8, 197, 4, 591, 117, 2, 21, 2374, 66, 597, 4, 20170, 4474, 1302, 3, 9, 2053, 1, 210, 766, 377, 8, 135, 6, 560, 1, 50, 4, 301, 147, 8, 3244, 81, 5, 3208, 6, 7, 507, 198, 2, 1, 852, 210, 766, 377, 31, 68, 757, 79, 17, 235, 15, 172, 10, 1939, 251, 410, 20, 726, 574, 4, 179, 1568, 1047, 44, 17, 300, 15, 172, 10, 890, 2970, 44, 17, 520, 15, 172, 10, 1117, 1242, 4, 70730, 194, 335, 12040, 34, 3944, 50, 34, 70731, 44, 17, 1037, 15, 172, 10, 1581, 5, 875, 37, 4, 188, 563, 8, 135, 6, 618, 679, 349, 6, 1, 1753, 8, 191, 61, 2, 8, 7, 17645, 3, 233, 1248, 12729, 946, 41, 68, 1045, 51, 8, 20170, 81, 97, 1, 809, 540, 405, 3, 1, 70, 128, 786, 102, 54, 2525, 4, 63, 8, 81, 5, 233, 54, 8, 3208, 2, 84, 3, 11265, 5, 50551, 720, 1, 1833, 153, 6, 2349, 227, 53, 63, 4, 12729, 946, 2, 8, 135, 6, 428, 1, 1968, 4, 12729, 946, 8, 3244, 81, 5, 3208, 3, 5542, 1348, 3, 240, 147, 1241, 18, 120, 18, 2011, 11265, 3, 19, 2396, 676, 2], [2509, 11, 20, 134, 211, 6, 205, 1, 40, 4, 125, 140, 2, 890, 2509, 798, 17, 594, 3, 140, 2749, 12, 22, 23, 11821, 15, 8, 110, 140, 3003, 11, 7, 579, 162, 2, 8, 13, 25, 3, 9, 77, 7, 131, 3, 679, 5, 771, 155, 10, 890, 33736, 2509, 798, 12, 4027, 54, 93, 75, 336, 17, 406, 3, 984, 70, 5, 13264, 70, 15, 2, 1, 210, 4, 24, 155, 11, 1, 40, 4, 169, 955, 9015, 491, 12, 6490, 1, 578, 4, 70, 9015, 253, 140, 2749, 3, 244, 12, 1, 69, 4, 890, 33736, 2509, 22, 23, 959, 6, 1, 69, 4, 683, 415, 2041, 838, 5, 532, 9015, 2, 9, 31, 295, 24, 155, 8, 7, 386, 186, 1331, 24414, 28, 6174, 635, 278, 6, 223, 2041, 2, 9, 31, 328, 1, 186, 6, 7, 110, 140, 871, 3, 12245, 1535, 1451, 4, 140, 3, 4162, 4, 5008, 6989, 587, 8, 1, 3523, 2926, 17, 406, 3, 3523, 21336, 3, 3523, 12989, 3, 4406, 9665, 3, 70732, 3, 3349, 15, 2, 55, 3, 60, 27, 102, 87, 248, 14, 1117, 2509, 3, 24414, 113, 2820, 10, 140, 2749, 12, 22, 23, 3, 89, 31, 43, 605, 68, 3, 11821, 8, 7, 211, 415, 6, 119, 2509, 12, 7710, 8, 1, 140, 871, 2, 24, 36, 56, 12, 24414, 22, 259, 70733, 33736, 2509, 798, 8, 300, 3048, 3, 5, 17646, 445, 145, 7112, 18, 1668, 798, 21, 52, 3, 5, 1748, 102, 1373, 4, 33736, 2509, 491, 8, 1395, 2509, 3168, 3, 32, 18, 3561, 9113, 342, 3, 5856, 2945, 1273, 3, 4427, 9312, 3, 2672, 1255, 409, 3, 938, 2], [2410, 11, 1, 325, 1865, 4, 743, 58, 29, 2004, 2737, 306, 2, 1977, 87, 11266, 478, 3, 1, 2410, 4, 20, 363, 34, 7, 361, 11, 1310, 39, 885, 1, 655, 46, 58, 218, 8, 7, 268, 308, 2, 35, 2763, 3, 1, 655, 52, 118, 37, 199, 218, 19, 307, 524, 8397, 2, 84, 3, 1, 1984, 4, 1, 218, 3305, 11, 3585, 10, 20, 4456, 609, 5, 558, 2, 13, 36, 8, 7, 1344, 124, 559, 1, 212, 218, 19, 4269, 524, 8, 369, 34, 8, 797, 21, 60, 52, 89, 1, 48, 19, 3813, 6, 2732, 10, 2699, 1, 52, 10, 2673, 5, 10452, 162, 315, 2, 162, 11, 396, 4, 1, 161, 5, 6, 1314, 1, 162, 76, 1653, 45, 3, 12, 90, 23, 4444, 16, 87, 1385, 353, 3, 11, 72, 2, 9, 77, 1239, 6, 2349, 1, 162, 45, 12, 90, 284, 8, 450, 20, 101, 6, 574, 5, 428, 75, 58, 2410, 8, 1, 1365, 308, 2]]\n",
      "1530\n",
      "57\n",
      "used\n",
      "[1, 57, 25, 1419, 59, 5163, 494, 22, 23, 484, 27, 20, 209, 110, 502, 4, 311, 140, 2, 1, 153, 4, 5163, 494, 22, 284, 6, 1290, 73, 5302, 2, 10, 357, 3, 9, 22, 4217, 343, 1, 314, 4, 1, 5302, 1908, 3, 34, 9, 22, 545, 54, 431, 1379, 39, 14, 1, 153, 2]\n",
      "[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# run some checks\n",
    "print(train_sequences[:5])\n",
    "\n",
    "# print(tokenizer.word_index)\n",
    "\n",
    "# find the max length of the sequences\n",
    "max_length = max([len(seq) for seq in train_sequences])\n",
    "print(max_length)\n",
    "\n",
    "# find index of a word \n",
    "print(tokenizer.word_index['present'])\n",
    "# find word at index \n",
    "print(tokenizer.index_word[51])\n",
    "\n",
    "print(train_sequences[0])\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 350, 100)          14339300  \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, 350, 128)          84480     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirecti  (None, 128)               98816     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 350)               45150     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14567746 (55.57 MB)\n",
      "Trainable params: 14567746 (55.57 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build bi-LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index)+1, output_dim=100, input_length=MAX_LEN))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(MAX_LEN, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1563/1563 [==============================] - 322s 203ms/step - loss: 0.0661 - accuracy: 0.0958\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 302s 193ms/step - loss: 0.0583 - accuracy: 0.1529\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 297s 190ms/step - loss: 0.0566 - accuracy: 0.2473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2d11730d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(train_x, train_y, batch_size=32, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 96ms/step\n",
      "pred: ['massive']\n",
      "actual: asic, cad, eda, layout, logic, mooc, vlsi\n",
      "\n",
      "\n",
      "pred: ['and']\n",
      "actual: attitudes, e-participation, gamification, public participation, usage behavior\n",
      "\n",
      "\n",
      "pred: []\n",
      "actual: anonymous, conversation, cues, voting\n",
      "\n",
      "\n",
      "pred: []\n",
      "actual: electromagnetism, evolutionary algorithms, multi-objective optimization, resource-constrained project scheduling\n",
      "\n",
      "\n",
      "pred: ['the']\n",
      "actual: consciousness, constraint, creativity, digital fine art, freedom\n",
      "\n",
      "\n",
      "pred: []\n",
      "actual: energy use, feedback, interaction design, persuasive computing, sustainability, visualization\n",
      "\n",
      "\n",
      "pred: []\n",
      "actual: guided search, model checking, verification\n",
      "\n",
      "\n",
      "pred: []\n",
      "actual: xml, digital preservation, integration, web service\n",
      "\n",
      "\n",
      "pred: ['in', 'an']\n",
      "actual: architecture, software ecosystem, software product lines, variability modeling\n",
      "\n",
      "\n",
      "pred: ['although', 'sensor', 'networks']\n",
      "actual: 3d-localization, delaunay triangulation, map construction, rssi, terrain modeling, wireless sensor networks\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing prediction\n",
    "preds = model.predict(test_x[30:40])\n",
    "\n",
    "# print(preds[0])\n",
    "\n",
    "# print prediction\n",
    "for i in range(len(preds)):\n",
    "    print(\"pred:\", util.pred_to_keywords(preds[i], test_x[i], tokenizer))\n",
    "    print(\"actual:\", test_kws[i])\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
