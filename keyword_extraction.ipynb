{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import nltk\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "\n",
    "import util as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../dataset/SciHTC/train_title_abstract_keywords.csv'\n",
    "TEST_PATH = '../dataset/SciHTC/test_title_abstract_keywords.csv'\n",
    "DEV_PATH = '../dataset/SciHTC/dev_title_abstract_keywords.csv'\n",
    "\n",
    "MAX_LEN = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = util.read_data(TRAIN_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    The present paper discusses how clone sets can...\n",
      "1    Based on the important progresses made in info...\n",
      "2    Over the past two decades, a growing body of r...\n",
      "3     Refactoring is an important way to improve th...\n",
      "4    Productivity is the main concern of today's IT...\n",
      "Name: Abstract, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.head(5)['Abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s1 = \"i am cat\"\n",
    "# s2 = \"i am dog\"\n",
    "# s3 = \"i am human\"\n",
    "# sentences = [s1, s2, s3]    \n",
    "# tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "# tokenizer = tf.keras.preprocessing.text.Tokenizer(sentences)\n",
    "# tokenizer.fit_on_texts(sentences)\n",
    "# sequences = tokenizer.texts_to_sequences(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = util.tokenize_sentence(df.head(5)['Abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'present', 'paper', 'discusses', 'how', 'clone', 'sets', 'can', 'be', 'generated', 'from', 'an', 'very', 'large', 'amount', 'of', 'source', 'code', '.', 'The', 'knowledge', 'of', 'clone', 'sets', 'can', 'help', 'to', 'manage', 'software', 'asset', '.', 'For', 'example', ',', 'we', 'can', 'figure', 'out', 'the', 'state', 'of', 'the', 'asset', 'easier', ',', 'or', 'we', 'can', 'build', 'more', 'useful', 'libraries', 'based', 'on', 'the', 'knowledge', '.'], ['Based', 'on', 'the', 'important', 'progresses', 'made', 'in', 'information', 'retrieval', '(', 'IR', ')', 'in', 'terms', 'of', 'theoretical', 'models', 'and', 'evaluations', ',', 'more', 'and', 'more', 'attention', 'has', 'recently', 'been', 'paid', 'to', 'the', 'research', 'in', 'domain', 'specific', 'IR', ',', 'as', 'evidenced', 'by', 'the', 'organization', 'of', 'Genomics', 'and', 'Legal', 'tracks', 'in', 'TREC', '(', 'Text', 'REtrieval', 'Conference', ')', '.', 'We', 'think', 'that', 'now', 'is', 'the', 'right', 'time', 'to', 'carry', 'out', 'large', 'scale', 'evaluations', 'on', 'chemistry', 'datasets', 'in', 'order', 'to', 'promote', 'the', 'research', 'in', 'chemical', 'IR', 'in', 'general', 'and', 'chemical', 'patent', 'IR', 'in', 'particular', '.', 'Accordingly', ',', 'we', 'propose', 'the', 'organization', 'of', 'a', 'chemical', 'IR', 'track', 'in', 'TREC', 'in', 'order', 'to', 'address', 'the', 'challenges', 'in', 'chemical', 'and', 'patent', 'IR', '.', 'In', 'this', 'position', 'paper', ',', 'we', 'present', 'the', 'research', 'questions', 'we', 'will', 'address', 'in', 'the', 'proposed', 'track', ',', 'our', 'initial', 'plan', 'of', 'the', 'proposed', 'track', ',', 'and', 'the', 'kind', 'of', 'search', 'tasks', 'we', 'propose', 'for', 'the', 'track', '.', 'We', 'focus', 'on', 'the', 'design', 'of', 'a', 'new', 'chemical', 'entity', 'search', 'task', 'consisting', 'of', 'two', 'sub-tasks', ',', 'i.e.', ',', 'chemical', 'entity', 'search', 'and', 'chemical', 'entity', 'relation', 'search', '.'], ['Over', 'the', 'past', 'two', 'decades', ',', 'a', 'growing', 'body', 'of', 'research', 'in', 'Borderline', 'Personality', 'Disorder', '(', 'BPD', ')', 'has', 'been', 'accumulated', 'using', 'computer-assisted', 'infield', 'methods', 'to', 'assess', 'self-report', 'data', ',', 'physiological', 'data', 'as', 'well', 'as', 'environmental', 'aspects', 'of', 'BPD', 'patients', 'in', 'everyday', 'life', 'situations', '.', 'In', 'psychological', 'and', 'psychiatric', 'research', ',', 'the', 'assessment', 'method', 'uniting', 'all', 'these', 'aspects', 'of', 'assessment', 'has', 'been', 'coined', '``', 'Ambulatory', 'Assessment', \"''\", '.', 'This', 'method', 'combines', 'many', 'advantages', 'besides', 'real-life', 'and', 'real-time', 'assessment', ',', 'e.g', '.', 'objective', 'and', 'unobtrusive', 'assessments', 'as', 'well', 'as', 'assessments', 'of', 'setting-', 'or', 'context-specificities', '.', 'A', 'seminal', 'field', 'of', 'application', 'is', 'the', 'use', 'of', 'electronic', 'devices', 'as', 'therapeutic', 'tools', ',', 'giving', 'interactive', 'feedback', 'to', 'patients', ',', 'i.e', '.', 'providing', 'interventions', 'at', 'the', 'very', 'moments', 'when', 'patients', 'are', 'most', 'in', 'need', 'of', 'additional', 'support', '.', 'By', 'giving', 'two', 'examples', 'of', 'BPD', 'diagnostic', 'criteria', ',', 'we', 'outline', 'the', 'key', 'technical', 'needs', 'in', 'order', 'to', 'take', 'the', 'use', 'of', 'sensor', 'technology', 'in', 'psychological', 'research', 'and', 'therapy', 'to', 'a', 'higher', 'level', '.', 'The', 'following', 'key', 'technical', 'needs', 'have', 'been', 'identified', ':', '(', '1', ')', 'tools', 'for', 'emotion', 'detection', 'providing', 'an', 'objective', 'measure', 'of', 'current', 'emotional', 'states', ';', '(', '2', ')', 'tools', 'for', 'identifying', 'persons', ';', '(', '3', ')', 'tools', 'for', 'detecting', 'instances', 'of', 'self-destructive', 'behavior', 'like', 'alcohol', 'or', 'drug', 'use', 'or', 'speeding/reckless-driving', ';', '(', '4', ')', 'tools', 'for', 'integrating', 'and', 'analyzing', 'information', 'of', 'various', 'sensors', 'in', 'order', 'to', 'give', 'automated', 'feedback', 'to', 'the', 'patient', 'in', 'real', 'time', '.', 'In', 'a', 'nutshell', ',', 'even', 'though', 'Ambulatory', 'Assessment', 'has', 'been', 'successfully', 'used', 'in', 'BPD', 'research', 'over', 'the', 'past', 'few', 'years', ',', 'the', 'method', 'may', 'offer', 'many', 'more', 'options', 'of', 'applications', 'in', 'research', 'and', 'even', 'more', 'in', 'therapy', '.', 'However', ',', 'psychologists', 'and', 'psychiatrists', 'lack', 'the', 'technological', 'knowledge', 'to', 'realize', 'possible', 'new', 'applications', 'of', 'Ambulatory', 'Assessment', '.', 'In', 'order', 'to', 'increase', 'the', 'usefulness', 'of', 'Ambulatory', 'Assessment', 'in', 'psychological', 'research', 'and', 'therapy', ',', 'multidisciplinary', 'teams', ',', 'including', 'technology', 'experts', 'as', 'well', 'as', 'clinical', 'psychologists', ',', 'are', 'clearly', 'needed', '.'], ['Refactoring', 'is', 'an', 'important', 'way', 'to', 'improve', 'the', 'design', 'of', 'existing', 'code', '.', 'Identifying', 'refactoring', 'opportunities', '(', 'i.e.', ',', 'code', 'fragments', 'that', 'can', 'be', 'refactored', ')', 'in', 'large', 'code', 'bases', 'is', 'a', 'challenging', 'task', '.', 'In', 'this', 'paper', ',', 'we', 'propose', 'a', 'novel', ',', 'automated', 'and', 'scalable', 'technique', 'for', 'identifying', 'cross-function', 'refactoring', 'opportunities', 'that', 'span', 'more', 'than', 'one', 'function', '(', 'e.g.', ',', 'Extract', 'Method', 'and', 'Inline', 'Method', ')', '.', 'The', 'key', 'of', 'our', 'technique', 'is', 'the', 'design', 'of', 'efficient', 'vector', 'inlining', 'operations', 'that', 'emulate', 'the', 'effect', 'of', 'method', 'inlining', 'among', 'code', 'fragments', ',', 'so', 'that', 'the', 'problem', 'of', 'identifying', 'cross-function', 'refactoring', 'can', 'be', 'reduced', 'to', 'the', 'problem', 'of', 'finding', 'similar', 'vectors', 'before', 'and', 'after', 'inlining', '.', 'We', 'have', 'implemented', 'our', 'technique', 'in', 'a', 'prototype', 'tool', 'named', 'ReDex', 'which', 'encodes', 'Java', 'programs', 'to', 'particular', 'vectors', '.', 'We', 'have', 'applied', 'the', 'tool', 'to', 'a', 'large', 'code', 'base', ',', '4.5', 'million', 'lines', 'of', 'code', ',', 'comprising', 'of', '200', 'bundle', 'projects', 'in', 'the', 'Eclipse', 'ecosystem', '(', 'e.g.', ',', 'Eclipse', 'JDT', ',', 'Eclipse', 'PDE', ',', 'Apache', 'Commons', ',', 'Hamcrest', ',', 'etc.', ')', '.', 'Also', ',', 'different', 'from', 'many', 'other', 'studies', 'on', 'detecting', 'refactoring', ',', 'ReDex', 'only', 'searches', 'for', 'code', 'fragments', 'that', 'can', 'be', ',', 'but', 'have', 'not', 'yet', 'been', ',', 'refactored', 'in', 'a', 'way', 'similar', 'to', 'some', 'refactoring', 'that', 'happened', 'in', 'the', 'code', 'base', '.', 'Our', 'results', 'show', 'that', 'ReDex', 'can', 'find', '277', 'cross-function', 'refactoring', 'opportunities', 'in', '2', 'minutes', ',', 'and', '223', 'cases', 'were', 'labelled', 'as', 'true', 'opportunities', 'by', 'users', ',', 'and', 'cover', 'many', 'categories', 'of', 'cross-function', 'refactoring', 'operations', 'in', 'classical', 'refactoring', 'books', ',', 'such', 'as', 'Self', 'Encapsulate', 'Field', ',', 'Decompose', 'Conditional', 'Expression', ',', 'Hide', 'Delegate', ',', 'Preserve', 'Whole', 'Object', ',', 'etc', '.'], ['Productivity', 'is', 'the', 'main', 'concern', 'of', 'today', \"'s\", 'IT', 'enabled', 'office', 'environments', '.', 'Besides', 'other', 'inter-related', 'factors', ',', 'the', 'productivity', 'of', 'an', 'individual', 'or', 'a', 'group', 'is', 'mainly', 'based', 'upon', 'the', 'end', 'user', \"'s\", 'tasks', 'in', 'a', 'specific', 'domain', '.', 'These', 'days', ',', 'the', 'end', 'users', \"'\", 'information', 'management', 'tasks', 'are', 'being', 'performed', 'semi-automatically', '.', 'However', ',', 'the', 'record', 'of', 'the', 'tasks', 'maintained', 'is', 'insufficient', 'for', 'an', 'exhaustive', 'tracking', 'and', 'monitoring', '.', 'This', 'results', 'in', 'a', 'situation', 'where', 'although', 'the', 'same', 'tasks', 'are', 'repeatedly', 'performed', 'in', 'parallel', 'or', 'in', 'sequence', 'by', 'different', 'users', 'but', 'the', 'systems', 'are', 'unable', 'to', 'coordinate', 'for', 'helping', 'the', 'users', 'for', 'smooth', 'and', 'swift', 'task', 'execution', '.', 'Task', 'is', 'part', 'of', 'the', 'context', 'and', 'to', 'track', 'the', 'task', 'its', 'conceptual', 'model', ',', 'that', 'will', 'be', 'interconnected', 'with', 'other', 'contextual', 'components', ',', 'is', 'proposed', '.', 'We', 'propose', 'ontology', 'to', 'realize', 'the', 'task', 'model', 'that', 'will', 'help', 'in', 'making', 'an', 'application', 'to', 'measure', 'and', 'increase', 'one', \"'s\", 'productivity', 'in', 'the', 'academic', 'domain', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(abstract)\n",
    "sequences = tokenizer.texts_to_sequences(abstract)\n",
    "train_x = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_LEN, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df.head(5)['Keywords'][1]))\n",
    "# remove [, ], ', and split by comma\n",
    "# keywords = [keywords.replace('[', '').replace(']', '').replace('\\'', '').split(', ') for keywords in df.head(5)['Keywords']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(keywords)\n",
    "\n",
    "# apply clean_keywords to each row of df['Keywords']\n",
    "df['Keywords'] = df['Keywords'].apply(util.clean_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [[code, clone], [reengineering, for, libraries]]\n",
      "1    [[chemical, information, retrieval], [chemistr...\n",
      "2    [[ambulatory, assessment], [borderline, person...\n",
      "3    [[refactoring], [software, evolution], [vector...\n",
      "4    [[information, context], [productivity], [task...\n",
      "Name: Keywords, dtype: object\n",
      "[['code', 'clone'], ['reengineering', 'for', 'libraries']]\n",
      "[['chemical', 'information', 'retrieval'], ['chemistry'], ['patent', 'retrieval'], ['prior', 'art', 'search']]\n",
      "[['ambulatory', 'assessment'], ['borderline', 'personality', 'disorder'], ['clinical', 'psychology'], ['everyday', 'life'], ['interactive', 'feedback'], ['real-time', 'data', 'interpretation'], ['unobtrusive', 'objective', 'assessment']]\n",
      "[['refactoring'], ['software', 'evolution'], ['vector-based', 'representation']]\n",
      "[['information', 'context'], ['productivity'], ['task'], ['task', 'modeling']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keywords = df.head(5)['Keywords']\n",
    "print(keywords)\n",
    "\n",
    "# convert keywords to list of list\n",
    "for each in keywords:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw = [['code', 'clone'], ['reengineering', 'for', 'libraries']]\n",
    "kw_seq = tokenizer.texts_to_sequences(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16, 77], [10, 151]]\n"
     ]
    }
   ],
   "source": [
    "print(kw_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def keywords_marking_exact(keyword_phrases: list, sequence: list, max_len: int, tokenizer: Tokenizer):\n",
    "    '''\n",
    "    Mark a sequence of tokens for the exact keyword phrases. \n",
    "    If the keyword phrase as a whole is not in the sequence, it will not be marked.\n",
    "    \n",
    "    params:\n",
    "        keyword_phrases: a list of keyword phrases, each phrase is a list of tokens\n",
    "    return:\n",
    "        a list of binary labels of the same length as input sequence, 1 for keyword phrases, 0 for others\n",
    "    '''\n",
    "    print(\"phrases\", keyword_phrases)\n",
    "    print(\">>>\")\n",
    "    binary_labels = [0] * max_len\n",
    "\n",
    "    # for phrase in keyword_phrases:\n",
    "    #     if phrase == []:\n",
    "    #         continue\n",
    "    #     print(\"phrase\", phrase)\n",
    "    #     # convert phrase tokens to sequence tokens\n",
    "    #     phrase_tokens = tokenizer.texts_to_sequences(phrase) # [0] because texts_to_sequences returns a list of lists\n",
    "    #     print(\"phrase tokens\", phrase_tokens)\n",
    "    #     print()\n",
    "    #     # see if the phrase is in the sequence\n",
    "    #     for i in range(len(sequence) - len(phrase_tokens) + 1):\n",
    "    #         if sequence[i:i+len(phrase_tokens)] == phrase_tokens: # matching the whole phrase\n",
    "    #             binary_labels[i:i+len(phrase_tokens)] = [1] * len(phrase_tokens)\n",
    "\n",
    "    phrase_tokens = tokenizer.texts_to_sequences(keyword_phrases)\n",
    "    print(\"phrase tokens\", phrase_tokens)\n",
    "\n",
    "    return binary_labels\n",
    "\n",
    "def keywords_marking(keywords: list, sequences: list, max_len: int, tokenizer: Tokenizer):\n",
    "    binary_labels = []\n",
    "    for i in range(len(keywords)):\n",
    "        binary_labels.append(keywords_marking_exact(keywords[i], sequences[i], max_len, tokenizer))\n",
    "    return binary_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 76, 51, 138, 139, 77, 78, 19, 29, 140, 79, 22, 80, 36, 141, 4, 142, 16, 5, 1, 52, 4, 77, 78, 19, 81, 7, 143, 144, 82, 5, 10, 145, 2, 9, 19, 146, 83, 1, 147, 4, 1, 82, 148, 2, 23, 9, 19, 149, 24, 150, 151, 53, 30, 1, 52, 5], [53, 30, 1, 84, 152, 153, 3, 54, 85, 11, 25, 12, 3, 154, 4, 155, 156, 6, 86, 2, 24, 6, 24, 157, 37, 158, 26, 159, 7, 1, 17, 3, 55, 87, 25, 2, 13, 160, 38, 1, 88, 4, 161, 6, 162, 163, 3, 89, 11, 164, 85, 165, 12, 5, 9, 166, 14, 167, 15, 1, 168, 90, 7, 169, 83, 36, 170, 86, 30, 171, 172, 3, 31, 7, 173, 1, 17, 3, 20, 25, 3, 174, 6, 20, 91, 25, 3, 92, 5, 175, 2, 9, 39, 1, 88, 4, 8, 20, 25, 32, 3, 89, 3, 31, 7, 93, 1, 176, 3, 20, 6, 91, 25, 5, 3, 40, 177, 51, 2, 9, 76, 1, 17, 178, 9, 56, 93, 3, 1, 57, 32, 2, 41, 179, 180, 4, 1, 57, 32, 2, 6, 1, 181, 4, 42, 33, 9, 39, 10, 1, 32, 5, 9, 182, 30, 1, 58, 4, 8, 94, 20, 59, 42, 27, 183, 4, 60, 184, 2, 95, 2, 20, 59, 42, 6, 20, 59, 185, 42, 5], [96, 1, 97, 60, 186, 2, 8, 187, 188, 4, 17, 3, 189, 190, 191, 11, 43, 12, 37, 26, 192, 193, 194, 195, 196, 7, 197, 198, 98, 2, 199, 98, 13, 61, 13, 200, 99, 4, 43, 62, 3, 201, 202, 203, 5, 3, 63, 6, 204, 17, 2, 1, 21, 28, 205, 206, 100, 99, 4, 21, 37, 26, 207, 208, 44, 21, 209, 5, 40, 28, 210, 45, 211, 101, 212, 6, 213, 21, 2, 214, 5, 102, 6, 215, 103, 13, 61, 13, 103, 4, 216, 23, 217, 5, 8, 218, 104, 4, 105, 15, 1, 64, 4, 219, 220, 13, 221, 34, 2, 106, 222, 107, 7, 62, 2, 223, 5, 108, 224, 225, 1, 80, 226, 227, 62, 35, 228, 3, 229, 4, 230, 231, 5, 38, 106, 60, 232, 4, 43, 233, 234, 2, 9, 235, 1, 65, 109, 110, 3, 31, 7, 236, 1, 64, 4, 237, 111, 3, 63, 17, 6, 66, 7, 8, 238, 239, 5, 1, 240, 65, 109, 110, 46, 26, 241, 242, 11, 243, 12, 34, 10, 244, 245, 108, 22, 102, 112, 4, 246, 247, 248, 67, 11, 113, 12, 34, 10, 47, 249, 67, 11, 250, 12, 34, 10, 114, 251, 4, 252, 253, 254, 255, 23, 256, 64, 23, 257, 67, 11, 258, 12, 34, 10, 259, 6, 260, 54, 4, 261, 262, 3, 31, 7, 263, 115, 107, 7, 1, 264, 3, 265, 90, 5, 3, 8, 266, 2, 116, 267, 44, 21, 37, 26, 268, 269, 3, 43, 17, 96, 1, 97, 270, 271, 2, 1, 28, 272, 273, 45, 24, 274, 4, 117, 3, 17, 6, 116, 24, 3, 66, 5, 118, 2, 119, 6, 275, 276, 1, 277, 52, 7, 120, 278, 94, 117, 4, 44, 21, 5, 3, 31, 7, 121, 1, 279, 4, 44, 21, 3, 63, 17, 6, 66, 2, 280, 281, 2, 282, 111, 283, 13, 61, 13, 284, 119, 2, 35, 285, 286, 5], [18, 15, 22, 84, 122, 7, 287, 1, 58, 4, 288, 16, 5, 47, 18, 48, 11, 95, 2, 16, 68, 14, 19, 29, 123, 12, 3, 36, 16, 289, 15, 8, 290, 27, 5, 3, 40, 51, 2, 9, 39, 8, 291, 2, 115, 6, 292, 69, 10, 47, 49, 18, 48, 14, 293, 24, 294, 124, 295, 11, 125, 2, 296, 28, 6, 297, 28, 12, 5, 1, 65, 4, 41, 69, 15, 1, 58, 4, 298, 299, 70, 126, 14, 300, 1, 301, 4, 28, 70, 302, 16, 68, 2, 303, 14, 1, 127, 4, 47, 49, 18, 19, 29, 304, 7, 1, 127, 4, 305, 128, 129, 306, 6, 307, 70, 5, 9, 46, 308, 41, 69, 3, 8, 309, 130, 310, 71, 311, 312, 313, 314, 7, 92, 129, 5, 9, 46, 315, 1, 130, 7, 8, 36, 16, 131, 2, 316, 317, 318, 4, 16, 2, 319, 4, 320, 321, 322, 3, 1, 72, 323, 11, 125, 2, 72, 324, 2, 72, 325, 2, 326, 327, 2, 328, 2, 329, 12, 5, 330, 2, 132, 79, 45, 73, 331, 30, 114, 18, 2, 71, 332, 333, 10, 16, 68, 14, 19, 29, 2, 133, 46, 334, 335, 26, 2, 123, 3, 8, 122, 128, 7, 336, 18, 14, 337, 3, 1, 16, 131, 5, 41, 134, 338, 14, 71, 19, 339, 340, 49, 18, 48, 3, 113, 341, 2, 6, 342, 343, 344, 345, 13, 346, 48, 38, 50, 2, 6, 347, 45, 348, 4, 49, 18, 126, 3, 349, 18, 350, 2, 351, 13, 352, 353, 104, 2, 354, 355, 356, 2, 357, 358, 2, 359, 360, 361, 2, 362, 5], [74, 15, 1, 363, 364, 4, 365, 75, 366, 367, 368, 369, 5, 101, 73, 370, 371, 2, 1, 74, 4, 22, 372, 23, 8, 373, 15, 374, 53, 375, 1, 135, 376, 75, 33, 3, 8, 87, 55, 5, 100, 377, 2, 1, 135, 50, 378, 54, 379, 33, 35, 380, 136, 381, 5, 118, 2, 1, 382, 4, 1, 33, 383, 15, 384, 10, 22, 385, 386, 6, 387, 5, 40, 134, 3, 8, 388, 389, 390, 1, 391, 33, 35, 392, 136, 3, 393, 23, 3, 394, 38, 132, 50, 133, 1, 395, 35, 396, 7, 397, 10, 398, 1, 50, 10, 399, 6, 400, 27, 401, 5, 27, 15, 402, 4, 1, 403, 6, 7, 32, 1, 27, 404, 405, 137, 2, 14, 56, 29, 406, 407, 73, 408, 409, 2, 15, 57, 5, 9, 39, 410, 7, 120, 1, 27, 137, 14, 56, 81, 3, 411, 22, 105, 7, 112, 6, 121, 124, 75, 74, 3, 1, 412, 55, 5]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "phrases [['code', 'clone'], ['reengineering', 'for', 'libraries']]\n",
      ">>>\n",
      "phrase tokens [[16, 77], [10, 151]]\n",
      "phrases [['chemical', 'information', 'retrieval'], ['chemistry'], ['patent', 'retrieval'], ['prior', 'art', 'search']]\n",
      ">>>\n",
      "phrase tokens [[20, 54, 85], [171], [91, 85], [42]]\n",
      "phrases [['ambulatory', 'assessment'], ['borderline', 'personality', 'disorder'], ['clinical', 'psychology'], ['everyday', 'life'], ['interactive', 'feedback'], ['real-time', 'data', 'interpretation'], ['unobtrusive', 'objective', 'assessment']]\n",
      ">>>\n",
      "phrase tokens [[44, 21], [189, 190, 191], [284], [201, 202], [222, 107], [213, 98], [215, 102, 21]]\n",
      "phrases [['refactoring'], ['software', 'evolution'], ['vector-based', 'representation']]\n",
      ">>>\n",
      "phrase tokens [[18], [144], []]\n",
      "phrases [['information', 'context'], ['productivity'], ['task'], ['task', 'modeling']]\n",
      ">>>\n",
      "phrase tokens [[54, 403], [74], [27], [27]]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# keywords = util.tokenize_sentence(df.head(5)['Keywords'])\n",
    "print(len(df.head(5)['Keywords']))\n",
    "\n",
    "train_y = keywords_marking(keywords, sequences, 350, tokenizer)\n",
    "print(train_y[0])\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "        id                                              Title  \\\n",
      "0  1808918  Toward identifying inter-project clone sets fo...   \n",
      "1  1458577  A proposal for chemical information retrieval ...   \n",
      "2  2307865  What does psychology and psychiatry need from ...   \n",
      "3  2610394  Scalable detection of missed cross-function re...   \n",
      "4  1838054  Exploiting information context to improve prod...   \n",
      "\n",
      "                  Subtitle                                           Abstract  \\\n",
      "0                      NaN  The present paper discusses how clone sets can...   \n",
      "1                      NaN  Based on the important progresses made in info...   \n",
      "2  an end-user perspective  Over the past two decades, a growing body of r...   \n",
      "3                      NaN   Refactoring is an important way to improve th...   \n",
      "4                      NaN  Productivity is the main concern of today's IT...   \n",
      "\n",
      "                                            Keywords  \n",
      "0      ['code clone', 'reengineering for libraries']  \n",
      "1  ['chemical information retrieval', 'chemistry'...  \n",
      "2  ['ambulatory assessment', 'borderline personal...  \n",
      "3  ['refactoring', 'software evolution', 'vector-...  \n",
      "4  ['information context', 'productivity', 'task'...  \n"
     ]
    }
   ],
   "source": [
    "print(len(sequences))\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 76, 51, 138, 139, 77, 78, 19, 29, 140, 79, 22, 80, 36, 141, 4, 142, 16, 5, 1, 52, 4, 77, 78, 19, 81, 7, 143, 144, 82, 5, 10, 145, 2, 9, 19, 146, 83, 1, 147, 4, 1, 82, 148, 2, 23, 9, 19, 149, 24, 150, 151, 53, 30, 1, 52, 5], [53, 30, 1, 84, 152, 153, 3, 54, 85, 11, 25, 12, 3, 154, 4, 155, 156, 6, 86, 2, 24, 6, 24, 157, 37, 158, 26, 159, 7, 1, 17, 3, 55, 87, 25, 2, 13, 160, 38, 1, 88, 4, 161, 6, 162, 163, 3, 89, 11, 164, 85, 165, 12, 5, 9, 166, 14, 167, 15, 1, 168, 90, 7, 169, 83, 36, 170, 86, 30, 171, 172, 3, 31, 7, 173, 1, 17, 3, 20, 25, 3, 174, 6, 20, 91, 25, 3, 92, 5, 175, 2, 9, 39, 1, 88, 4, 8, 20, 25, 32, 3, 89, 3, 31, 7, 93, 1, 176, 3, 20, 6, 91, 25, 5, 3, 40, 177, 51, 2, 9, 76, 1, 17, 178, 9, 56, 93, 3, 1, 57, 32, 2, 41, 179, 180, 4, 1, 57, 32, 2, 6, 1, 181, 4, 42, 33, 9, 39, 10, 1, 32, 5, 9, 182, 30, 1, 58, 4, 8, 94, 20, 59, 42, 27, 183, 4, 60, 184, 2, 95, 2, 20, 59, 42, 6, 20, 59, 185, 42, 5], [96, 1, 97, 60, 186, 2, 8, 187, 188, 4, 17, 3, 189, 190, 191, 11, 43, 12, 37, 26, 192, 193, 194, 195, 196, 7, 197, 198, 98, 2, 199, 98, 13, 61, 13, 200, 99, 4, 43, 62, 3, 201, 202, 203, 5, 3, 63, 6, 204, 17, 2, 1, 21, 28, 205, 206, 100, 99, 4, 21, 37, 26, 207, 208, 44, 21, 209, 5, 40, 28, 210, 45, 211, 101, 212, 6, 213, 21, 2, 214, 5, 102, 6, 215, 103, 13, 61, 13, 103, 4, 216, 23, 217, 5, 8, 218, 104, 4, 105, 15, 1, 64, 4, 219, 220, 13, 221, 34, 2, 106, 222, 107, 7, 62, 2, 223, 5, 108, 224, 225, 1, 80, 226, 227, 62, 35, 228, 3, 229, 4, 230, 231, 5, 38, 106, 60, 232, 4, 43, 233, 234, 2, 9, 235, 1, 65, 109, 110, 3, 31, 7, 236, 1, 64, 4, 237, 111, 3, 63, 17, 6, 66, 7, 8, 238, 239, 5, 1, 240, 65, 109, 110, 46, 26, 241, 242, 11, 243, 12, 34, 10, 244, 245, 108, 22, 102, 112, 4, 246, 247, 248, 67, 11, 113, 12, 34, 10, 47, 249, 67, 11, 250, 12, 34, 10, 114, 251, 4, 252, 253, 254, 255, 23, 256, 64, 23, 257, 67, 11, 258, 12, 34, 10, 259, 6, 260, 54, 4, 261, 262, 3, 31, 7, 263, 115, 107, 7, 1, 264, 3, 265, 90, 5, 3, 8, 266, 2, 116, 267, 44, 21, 37, 26, 268, 269, 3, 43, 17, 96, 1, 97, 270, 271, 2, 1, 28, 272, 273, 45, 24, 274, 4, 117, 3, 17, 6, 116, 24, 3, 66, 5, 118, 2, 119, 6, 275, 276, 1, 277, 52, 7, 120, 278, 94, 117, 4, 44, 21, 5, 3, 31, 7, 121, 1, 279, 4, 44, 21, 3, 63, 17, 6, 66, 2, 280, 281, 2, 282, 111, 283, 13, 61, 13, 284, 119, 2, 35, 285, 286, 5], [18, 15, 22, 84, 122, 7, 287, 1, 58, 4, 288, 16, 5, 47, 18, 48, 11, 95, 2, 16, 68, 14, 19, 29, 123, 12, 3, 36, 16, 289, 15, 8, 290, 27, 5, 3, 40, 51, 2, 9, 39, 8, 291, 2, 115, 6, 292, 69, 10, 47, 49, 18, 48, 14, 293, 24, 294, 124, 295, 11, 125, 2, 296, 28, 6, 297, 28, 12, 5, 1, 65, 4, 41, 69, 15, 1, 58, 4, 298, 299, 70, 126, 14, 300, 1, 301, 4, 28, 70, 302, 16, 68, 2, 303, 14, 1, 127, 4, 47, 49, 18, 19, 29, 304, 7, 1, 127, 4, 305, 128, 129, 306, 6, 307, 70, 5, 9, 46, 308, 41, 69, 3, 8, 309, 130, 310, 71, 311, 312, 313, 314, 7, 92, 129, 5, 9, 46, 315, 1, 130, 7, 8, 36, 16, 131, 2, 316, 317, 318, 4, 16, 2, 319, 4, 320, 321, 322, 3, 1, 72, 323, 11, 125, 2, 72, 324, 2, 72, 325, 2, 326, 327, 2, 328, 2, 329, 12, 5, 330, 2, 132, 79, 45, 73, 331, 30, 114, 18, 2, 71, 332, 333, 10, 16, 68, 14, 19, 29, 2, 133, 46, 334, 335, 26, 2, 123, 3, 8, 122, 128, 7, 336, 18, 14, 337, 3, 1, 16, 131, 5, 41, 134, 338, 14, 71, 19, 339, 340, 49, 18, 48, 3, 113, 341, 2, 6, 342, 343, 344, 345, 13, 346, 48, 38, 50, 2, 6, 347, 45, 348, 4, 49, 18, 126, 3, 349, 18, 350, 2, 351, 13, 352, 353, 104, 2, 354, 355, 356, 2, 357, 358, 2, 359, 360, 361, 2, 362, 5], [74, 15, 1, 363, 364, 4, 365, 75, 366, 367, 368, 369, 5, 101, 73, 370, 371, 2, 1, 74, 4, 22, 372, 23, 8, 373, 15, 374, 53, 375, 1, 135, 376, 75, 33, 3, 8, 87, 55, 5, 100, 377, 2, 1, 135, 50, 378, 54, 379, 33, 35, 380, 136, 381, 5, 118, 2, 1, 382, 4, 1, 33, 383, 15, 384, 10, 22, 385, 386, 6, 387, 5, 40, 134, 3, 8, 388, 389, 390, 1, 391, 33, 35, 392, 136, 3, 393, 23, 3, 394, 38, 132, 50, 133, 1, 395, 35, 396, 7, 397, 10, 398, 1, 50, 10, 399, 6, 400, 27, 401, 5, 27, 15, 402, 4, 1, 403, 6, 7, 32, 1, 27, 404, 405, 137, 2, 14, 56, 29, 406, 407, 73, 408, 409, 2, 15, 57, 5, 9, 39, 410, 7, 120, 1, 27, 137, 14, 56, 81, 3, 411, 22, 105, 7, 112, 6, 121, 124, 75, 74, 3, 1, 412, 55, 5]]\n",
      "{'the': 1, ',': 2, 'in': 3, 'of': 4, '.': 5, 'and': 6, 'to': 7, 'a': 8, 'we': 9, 'for': 10, '(': 11, ')': 12, 'as': 13, 'that': 14, 'is': 15, 'code': 16, 'research': 17, 'refactoring': 18, 'can': 19, 'chemical': 20, 'assessment': 21, 'an': 22, 'or': 23, 'more': 24, 'ir': 25, 'been': 26, 'task': 27, 'method': 28, 'be': 29, 'on': 30, 'order': 31, 'track': 32, 'tasks': 33, 'tools': 34, 'are': 35, 'large': 36, 'has': 37, 'by': 38, 'propose': 39, 'this': 40, 'our': 41, 'search': 42, 'bpd': 43, 'ambulatory': 44, 'many': 45, 'have': 46, 'identifying': 47, 'opportunities': 48, 'cross-function': 49, 'users': 50, 'paper': 51, 'knowledge': 52, 'based': 53, 'information': 54, 'domain': 55, 'will': 56, 'proposed': 57, 'design': 58, 'entity': 59, 'two': 60, 'well': 61, 'patients': 62, 'psychological': 63, 'use': 64, 'key': 65, 'therapy': 66, ';': 67, 'fragments': 68, 'technique': 69, 'inlining': 70, 'redex': 71, 'eclipse': 72, 'other': 73, 'productivity': 74, \"'s\": 75, 'present': 76, 'clone': 77, 'sets': 78, 'from': 79, 'very': 80, 'help': 81, 'asset': 82, 'out': 83, 'important': 84, 'retrieval': 85, 'evaluations': 86, 'specific': 87, 'organization': 88, 'trec': 89, 'time': 90, 'patent': 91, 'particular': 92, 'address': 93, 'new': 94, 'i.e.': 95, 'over': 96, 'past': 97, 'data': 98, 'aspects': 99, 'these': 100, 'besides': 101, 'objective': 102, 'assessments': 103, 'field': 104, 'application': 105, 'giving': 106, 'feedback': 107, 'providing': 108, 'technical': 109, 'needs': 110, 'technology': 111, 'measure': 112, '2': 113, 'detecting': 114, 'automated': 115, 'even': 116, 'applications': 117, 'however': 118, 'psychologists': 119, 'realize': 120, 'increase': 121, 'way': 122, 'refactored': 123, 'one': 124, 'e.g.': 125, 'operations': 126, 'problem': 127, 'similar': 128, 'vectors': 129, 'tool': 130, 'base': 131, 'different': 132, 'but': 133, 'results': 134, 'end': 135, 'performed': 136, 'model': 137, 'discusses': 138, 'how': 139, 'generated': 140, 'amount': 141, 'source': 142, 'manage': 143, 'software': 144, 'example': 145, 'figure': 146, 'state': 147, 'easier': 148, 'build': 149, 'useful': 150, 'libraries': 151, 'progresses': 152, 'made': 153, 'terms': 154, 'theoretical': 155, 'models': 156, 'attention': 157, 'recently': 158, 'paid': 159, 'evidenced': 160, 'genomics': 161, 'legal': 162, 'tracks': 163, 'text': 164, 'conference': 165, 'think': 166, 'now': 167, 'right': 168, 'carry': 169, 'scale': 170, 'chemistry': 171, 'datasets': 172, 'promote': 173, 'general': 174, 'accordingly': 175, 'challenges': 176, 'position': 177, 'questions': 178, 'initial': 179, 'plan': 180, 'kind': 181, 'focus': 182, 'consisting': 183, 'sub-tasks': 184, 'relation': 185, 'decades': 186, 'growing': 187, 'body': 188, 'borderline': 189, 'personality': 190, 'disorder': 191, 'accumulated': 192, 'using': 193, 'computer-assisted': 194, 'infield': 195, 'methods': 196, 'assess': 197, 'self-report': 198, 'physiological': 199, 'environmental': 200, 'everyday': 201, 'life': 202, 'situations': 203, 'psychiatric': 204, 'uniting': 205, 'all': 206, 'coined': 207, '``': 208, \"''\": 209, 'combines': 210, 'advantages': 211, 'real-life': 212, 'real-time': 213, 'e.g': 214, 'unobtrusive': 215, 'setting-': 216, 'context-specificities': 217, 'seminal': 218, 'electronic': 219, 'devices': 220, 'therapeutic': 221, 'interactive': 222, 'i.e': 223, 'interventions': 224, 'at': 225, 'moments': 226, 'when': 227, 'most': 228, 'need': 229, 'additional': 230, 'support': 231, 'examples': 232, 'diagnostic': 233, 'criteria': 234, 'outline': 235, 'take': 236, 'sensor': 237, 'higher': 238, 'level': 239, 'following': 240, 'identified': 241, ':': 242, '1': 243, 'emotion': 244, 'detection': 245, 'current': 246, 'emotional': 247, 'states': 248, 'persons': 249, '3': 250, 'instances': 251, 'self-destructive': 252, 'behavior': 253, 'like': 254, 'alcohol': 255, 'drug': 256, 'speeding/reckless-driving': 257, '4': 258, 'integrating': 259, 'analyzing': 260, 'various': 261, 'sensors': 262, 'give': 263, 'patient': 264, 'real': 265, 'nutshell': 266, 'though': 267, 'successfully': 268, 'used': 269, 'few': 270, 'years': 271, 'may': 272, 'offer': 273, 'options': 274, 'psychiatrists': 275, 'lack': 276, 'technological': 277, 'possible': 278, 'usefulness': 279, 'multidisciplinary': 280, 'teams': 281, 'including': 282, 'experts': 283, 'clinical': 284, 'clearly': 285, 'needed': 286, 'improve': 287, 'existing': 288, 'bases': 289, 'challenging': 290, 'novel': 291, 'scalable': 292, 'span': 293, 'than': 294, 'function': 295, 'extract': 296, 'inline': 297, 'efficient': 298, 'vector': 299, 'emulate': 300, 'effect': 301, 'among': 302, 'so': 303, 'reduced': 304, 'finding': 305, 'before': 306, 'after': 307, 'implemented': 308, 'prototype': 309, 'named': 310, 'which': 311, 'encodes': 312, 'java': 313, 'programs': 314, 'applied': 315, '4.5': 316, 'million': 317, 'lines': 318, 'comprising': 319, '200': 320, 'bundle': 321, 'projects': 322, 'ecosystem': 323, 'jdt': 324, 'pde': 325, 'apache': 326, 'commons': 327, 'hamcrest': 328, 'etc.': 329, 'also': 330, 'studies': 331, 'only': 332, 'searches': 333, 'not': 334, 'yet': 335, 'some': 336, 'happened': 337, 'show': 338, 'find': 339, '277': 340, 'minutes': 341, '223': 342, 'cases': 343, 'were': 344, 'labelled': 345, 'true': 346, 'cover': 347, 'categories': 348, 'classical': 349, 'books': 350, 'such': 351, 'self': 352, 'encapsulate': 353, 'decompose': 354, 'conditional': 355, 'expression': 356, 'hide': 357, 'delegate': 358, 'preserve': 359, 'whole': 360, 'object': 361, 'etc': 362, 'main': 363, 'concern': 364, 'today': 365, 'it': 366, 'enabled': 367, 'office': 368, 'environments': 369, 'inter-related': 370, 'factors': 371, 'individual': 372, 'group': 373, 'mainly': 374, 'upon': 375, 'user': 376, 'days': 377, \"'\": 378, 'management': 379, 'being': 380, 'semi-automatically': 381, 'record': 382, 'maintained': 383, 'insufficient': 384, 'exhaustive': 385, 'tracking': 386, 'monitoring': 387, 'situation': 388, 'where': 389, 'although': 390, 'same': 391, 'repeatedly': 392, 'parallel': 393, 'sequence': 394, 'systems': 395, 'unable': 396, 'coordinate': 397, 'helping': 398, 'smooth': 399, 'swift': 400, 'execution': 401, 'part': 402, 'context': 403, 'its': 404, 'conceptual': 405, 'interconnected': 406, 'with': 407, 'contextual': 408, 'components': 409, 'ontology': 410, 'making': 411, 'academic': 412}\n",
      "330\n",
      "76\n",
      "paper\n"
     ]
    }
   ],
   "source": [
    "print(sequences)\n",
    "print(tokenizer.word_index)\n",
    "# find the max length of the sequences\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "print(max_length)\n",
    "# find index of the word \n",
    "print(tokenizer.word_index['present'])\n",
    "# find the word at index 51\n",
    "print(tokenizer.index_word[51])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
