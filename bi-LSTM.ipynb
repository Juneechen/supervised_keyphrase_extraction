{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/juneechen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juneechen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/juneechen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# import nltk\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "\n",
    "import util as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word embeddings from the gensim package\n",
    "import gensim.downloader\n",
    "\n",
    "# download the glove embeddings\n",
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../dataset/SciHTC/train_title_abstract_keywords.csv'\n",
    "TEST_PATH = '../dataset/SciHTC/test_title_abstract_keywords.csv'\n",
    "DEV_PATH = '../dataset/SciHTC/dev_title_abstract_keywords.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 350\n",
    "EMBEDDING_DIM = 50\n",
    "SAMPLE_SIZE = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_matrix shape: (21944, 50)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read train and test data\n",
    "train_df = util.read_data(TRAIN_PATH)\n",
    "test_df = util.read_data(TEST_PATH)\n",
    "\n",
    "input_cols = ['Title', 'Abstract']\n",
    "\n",
    "# process the data and sample some for testing; it changes to df in place. \n",
    "# Reload df if running again\n",
    "train_df = util.preprocess_data(train_df, input_cols, 'Keywords', sample_size=SAMPLE_SIZE)\n",
    "test_df = util.preprocess_data(test_df, input_cols, 'Keywords', sample_size=SAMPLE_SIZE)\n",
    "# test_df = util.preprocess_data(test_df, input_cols, 'Keywords')\n",
    "\n",
    "# set up the tokenizer\n",
    "tokenizer = util.setup_tokenizer(train_df, test_df, ['input_tokens', 'clean_kp'])\n",
    "\n",
    "# create embeddings matrix\n",
    "embeddings_matrix = util.get_embeddings_matrix(tokenizer, glove_vectors, EMBEDDING_DIM)\n",
    "\n",
    "# create the input array\n",
    "train_X, train_Y = util.create_input_array(train_df, 'input_tokens', 'clean_kp', tokenizer,\n",
    "                                           embeddings_matrix, EMBEDDING_DIM, MAX_LEN)\n",
    "\n",
    "test_X, test_Y = util.create_input_array(test_df, 'input_tokens', 'clean_kp', tokenizer,\n",
    "                                            embeddings_matrix, EMBEDDING_DIM, MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_matrix shape: (21944, 50)\n",
      "train_X shape: (1000, 350, 50)\n",
      "train_Y shape: (1000, 350)\n",
      "test_X shape: (1000, 350, 50)\n",
      "test_Y shape: (1000, 350)\n",
      "Number of words in the vocabulary: 21944\n",
      "original samples max length: 448\n"
     ]
    }
   ],
   "source": [
    "print(\"embeddings_matrix shape:\", embeddings_matrix.shape)\n",
    "print(\"train_X shape:\", train_X.shape)\n",
    "print(\"train_Y shape:\", train_Y.shape)\n",
    "print(\"test_X shape:\", test_X.shape)\n",
    "print(\"test_Y shape:\", test_Y.shape)\n",
    "\n",
    "# print(test_df.head(1))\n",
    "# print(test_Y[0])\n",
    "\n",
    "print(\"Number of words in the vocabulary:\", len(tokenizer.word_index))\n",
    "# print(tokenizer.index_word[0])\n",
    "\n",
    "# find the max length of the input sequences\n",
    "max_length = max([len(seq) for seq in train_df['input_tokens']])\n",
    "print(\"original samples max length:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_2 (Bidirecti  (None, 350, 128)          58880     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirecti  (None, 128)               98816     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 350)               45150     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 202846 (792.37 KB)\n",
      "Trainable params: 202846 (792.37 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build bi-LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(MAX_LEN, EMBEDDING_DIM)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(MAX_LEN, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "32/32 [==============================] - 4s 137ms/step - loss: 0.1278 - accuracy: 0.1350\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 4s 137ms/step - loss: 0.1277 - accuracy: 0.1350\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 4s 135ms/step - loss: 0.1277 - accuracy: 0.1350\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 4s 135ms/step - loss: 0.1276 - accuracy: 0.1970\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 4s 136ms/step - loss: 0.1276 - accuracy: 0.1880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x297e42010>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(train_X, train_Y, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 3s 75ms/step\n",
      "1000\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "# testing prediction\n",
    "preds = model.predict(test_X)\n",
    "\n",
    "print(len(preds))\n",
    "\n",
    "# print prediction\n",
    "pred_kws = util.pred_to_keywords(preds, test_df['input_tokens'].values)\n",
    "print(pred_kws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
