# Keyphrase Extraction with NLP models

## Introduction

> What did you do and why is it interesting?

## Datasets

### Sci-HTC

### set 2

## The Models

> How do your model(s) work? (explain) Point out any important details regarding your approaches to unknown words, training, decoding, etc.

### bi-LSTM

**Input**  
From our dataset, we extracted x training samples of ‘abstract’ and ‘keywords’ to be the input for the model. Before going into the model, each 'abstract' is tokenized and converted into a sequence of integer with the keras `Tokenizer`. Padding and truncation are applied to make all input sequences the same length.
To decide on the max length for setting the input, we mapped out the length distribution over the training samples. The max length is set to x because about x% of the train sample is within this length.
[insert some distribution graph]

```
# set up tokenizer, fit on both train and test data
tokenizer = tf.keras.preprocessing.text.Tokenizer()
tokenizer.fit_on_texts(train_abs)
tokenizer.fit_on_texts(test_abs)

# set up train sequences (with padding)
train_sequences = tokenizer.texts_to_sequences(train_abs)
train_x = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=MAX_LEN, padding='post')
```

Labels are generated by marking the input sequences for keywords. Each label is a binary sequence of the same length as the input sequence. `label_seq[i]` is set to 1 if `input_seq[i]` is a pre-defined keyword; otherwise `label_seq[i] = 0`.

**unknown words**

**output**

**inferrence**

**steps**

**how the model works internally**

### pre-trained BERT

## Experiments

### bi-LSTM

### pre-trained BERT

## Results

## Summary

## Reference
