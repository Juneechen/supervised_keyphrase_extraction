# Keyphrase Extraction with Neural Networks

## Introduction

> What did you do and why is it interesting?

## Datasets

### Sci-HTC

### set 2

## The Models

> How do your model(s) work? (explain) Point out any important details regarding your approaches to unknown words, training, decoding, etc.

### bi-LSTM

**Input**  
From our dataset, we extracted x training samples of ‘abstract’ and ‘keywords’ to be the input for the model. Before going into the model, each 'abstract' is tokenized and converted into contextual embeddings using the glove-wiki-gigaword-50 vectors. Padding and truncation are applied to embeddings for making all input sequences the same length.
To decide on the max length for setting the input, we mapped out the length distribution over the training samples. The max length is set to 350 because about x% of the train sample is within this length.
[insert some distribution graph]

Labels are generated by marking the input sequences for keywords. Each label is a binary sequence of the same length as the input list of embedding vectors. `label[i]` is set to 1 if `input_sequences[i]` is part of the pre-defined keyphrases; otherwise `label[i] = 0`.

**unknown words**
Unkown words are represented by a vector of 0s with the length of embeddings dimension.

**output**

**inferrence**

**steps**

**how the model works internally**

### pre-trained BERT

## Experiments & Results

### bi-LSTM

### pre-trained BERT

### comparison with statistical model (TF-IDF)

## Summary

## Reference
