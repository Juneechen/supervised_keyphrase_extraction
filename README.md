# Keyphrase Extraction with Neural Networks

## Introduction

> What did you do and why is it interesting?

## Datasets

### Sci-HTC

### set 2

> How do your model(s) work? (explain) Point out any important details regarding your approaches to unknown words, training, decoding, etc.

## The Model: bi-LSTM for Key-phrase Extraction

### Input & Pre-process

From our dataset, we extracted 18000 random training samples, and we concatenate the 'title' and ‘abstract’ to be the input for the model. An input sample looks like this:

    Title:      Toward identifying inter-project clone sets for building useful libraries
    Abstract:   The present paper discusses how clone sets can be generated from an very large amount of source code. The knowledge of clone sets can help to manage software asset. For example, we can figure out the state of the asset easier, or we can build more useful libraries based on the knowledge.

Each input sample is first normalized by converting all to lower-case, and removing special characters.

    # keeping non-alphabetic char, space, and hyphen
    text = re.sub('[^\w\s-]', '', text)

It is then further pre-processed by tokenizing, removing English stopwords, and applying lemmatization. These process is done using `nltk.corpus.stopwords`, `nltk.tokenize.word_tokenize`, and `nltk.stem.WordNetLemmatizer`. The pre-processed input from the above example looks like this:

    ['toward', 'identifying', 'inter-project', 'clone', 'set', 'building', 'useful', 'library', 'present', 'paper', 'discus', 'clone', 'set', 'generated', 'large', 'amount', 'source', 'code', 'knowledge', 'clone', 'set', 'help', 'manage', 'software', 'asset', 'example', 'figure', 'state', 'asset', 'easier', 'build', 'useful', 'library', 'based', 'knowledge']

Padding and truncation are applied to these input tokens for the sequential model. To decide on the max length for setting the input, we mapped out the length (number of tokens) distribution over the pre-processed input from our traning samples. The max length is eventually set to 250 because about 98% of the train samples is within this length. We did experiment with other length settings as well to see how that affect our results, and a few experiments will be presented in later section.

[insert some distribution graph]

The final input to our model is the padded input tokens converted to embedding vectors using contextual word embeddings `GloVe 50`.

**Unknown Words** are represented by a vector of 0s with the length of embeddings dimension.

**Label**  
Labels are generated by marking the input sequences with the pre-defined key-phrases provided in the dataset, in the form of `['code clone', 'reengineering for libraries']`. We applied the same process used for cleaning input to clean the phrases, so they are normalized into the same form as the input tokens. Each label is list of binary value of the same length as the padded input tokens. `label[i] = 1` if `input_tokens[i]` is part of the pre-defined key-phrases; otherwise `label[i] = 0`.

### Model Setup

@EshanW313 help describe the config used to train the final models.

### Output & Inferrence

The output predictions from the model is a list of probabilities of the same length as the padded input tokens. `preds[i]` represents the likelihood of `input_tokens[i]` being a keyword. The predicted probabilities are in the range of 0 to 1, we initially used a threshold of 0.5 to convert them into binary predictions, however almost all of the labels becomes 0. We investigated the original predictions and learned that almost all predicted probabilities is below 0.5. The threshold is eventually lowered to between 0.3 and 0.35 for our later experiments.

The orinal `input_tokens[i]` is then used to convert `preds[i]` back to keywords: `input_tokens[i]` is a predicted keyword if `binary_preds[i]` = 1.

### Experiments & Results

Overall, despite the low accurary score, we are still able to see many overlaps between the pre-defined key phrases and our predicted keywords. the result from a few experiments are shown below:

**Experiment 1:**  
training sample=10000, embedding dim=50, maxLen=350, epoch=5, batchSize=32

    # 4m 17.5s
    Epoch 1/5
    2023-12-05 17:39:38.704371: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.
    313/313 [==============================] - 60s 168ms/step - loss: 0.1556 - accuracy: 0.1800
    Epoch 2/5
    313/313 [==============================] - 46s 148ms/step - loss: 0.1257 - accuracy: 0.1904
    Epoch 3/5
    313/313 [==============================] - 45s 145ms/step - loss: 0.1256 - accuracy: 0.1928
    Epoch 4/5
    313/313 [==============================] - 51s 163ms/step - loss: 0.1256 - accuracy: 0.1773
    Epoch 5/5
    313/313 [==============================] - 53s 168ms/step - loss: 0.1256 - accuracy: 0.1775

Some predictions it gives by using a threshold of 0.3 when converting probablity distribution to binary prediction:

    actual: ['spectral learning', 'transfer learning']
    predicted: ['domain-transfer', 'learning', 'traditional', 'spectral', 'classification']
    actual: ['case study', 'verification and validation', 'very-large-scale software system']
    predicted: ['study', 'testing', 'commissioning', 'operation']
    actual: ['assistive technology', 'context aware mobile system', 'end-user programming', 'technology abandonment']
    predicted: ['user', 'programming', 'context', 'responsiveness']
    actual: ['collision avoidance', 'human-like character', 'motion planning', 'multiagent system', 'object grasping']
    predicted: ['conquer', 'freewill', 'framework', 'aim']
    actual: ['human-robot interaction', 'mood induction procedure']
    predicted: ['language', 'mood', 'induction', 'procedure']

### Comparison with Statistical Approach (TF-IDF)

## pre-trained BERT for

## Summary

## Reference
